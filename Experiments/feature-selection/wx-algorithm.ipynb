{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from wx_hyperparam import WxHyperParameter\n",
    "from wx_core import wx_slp, wx_mlp, connection_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(num_cls=2):\n",
    "    train_num = 100\n",
    "    test_num = 100\n",
    "    input_dim = 20000\n",
    "    num_cls = num_cls\n",
    "    if num_cls < 2:\n",
    "        return\n",
    "\n",
    "    x_train = np.random.random((train_num, input_dim))\n",
    "    y_train = to_categorical(np.random.randint(num_cls, size=(train_num, 1)), num_classes=num_cls)\n",
    "\n",
    "    x_test = np.random.random((test_num, input_dim))\n",
    "    y_test = to_categorical(np.random.randint(num_cls, size=(test_num, 1)), num_classes=num_cls)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def StandByCol(data_frame, unused_cols=[]):\n",
    "    unused_cols_list = []\n",
    "    for col_ in unused_cols:\n",
    "        unused_cols_list.append(data_frame[col_])\n",
    "    data_frame = data_frame.drop(unused_cols,axis=1)\n",
    "\n",
    "    data_frame = data_frame.astype(float)\n",
    "    data_frame.fillna(0, inplace=True)\n",
    "    data_frame = data_frame.apply(lambda x: ((x-x.mean())/x.std()), axis=0)\n",
    "\n",
    "    for n,col_ in enumerate(unused_cols):\n",
    "        data_frame[col_] = unused_cols_list[n]\n",
    "\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from actual datas\n",
    "df_data = pd.read_csv('input_data_1.csv')\n",
    "f_names = df_data['fnames'].values\n",
    "df_data = df_data.drop(['fnames'],axis=1)\n",
    "id_names = df_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample0</th>\n",
       "      <th>Sample1</th>\n",
       "      <th>Sample2</th>\n",
       "      <th>Sample3</th>\n",
       "      <th>Sample4</th>\n",
       "      <th>Sample5</th>\n",
       "      <th>Sample6</th>\n",
       "      <th>Sample7</th>\n",
       "      <th>Sample8</th>\n",
       "      <th>Sample9</th>\n",
       "      <th>Sample10</th>\n",
       "      <th>Sample11</th>\n",
       "      <th>Sample12</th>\n",
       "      <th>Sample13</th>\n",
       "      <th>Sample14</th>\n",
       "      <th>Sample15</th>\n",
       "      <th>Sample16</th>\n",
       "      <th>Sample17</th>\n",
       "      <th>Sample18</th>\n",
       "      <th>Sample19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.231762</td>\n",
       "      <td>-0.247491</td>\n",
       "      <td>-0.248976</td>\n",
       "      <td>-0.202346</td>\n",
       "      <td>-0.186685</td>\n",
       "      <td>-0.223989</td>\n",
       "      <td>-0.179934</td>\n",
       "      <td>-0.214987</td>\n",
       "      <td>-0.166885</td>\n",
       "      <td>-0.280402</td>\n",
       "      <td>-0.089822</td>\n",
       "      <td>-0.124037</td>\n",
       "      <td>-0.114895</td>\n",
       "      <td>-0.098585</td>\n",
       "      <td>-0.166281</td>\n",
       "      <td>-0.111795</td>\n",
       "      <td>-0.179398</td>\n",
       "      <td>-0.124769</td>\n",
       "      <td>-0.115809</td>\n",
       "      <td>-0.115490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.238106</td>\n",
       "      <td>-0.284280</td>\n",
       "      <td>-0.262749</td>\n",
       "      <td>-0.237539</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.231590</td>\n",
       "      <td>-0.217126</td>\n",
       "      <td>-0.219176</td>\n",
       "      <td>-0.167156</td>\n",
       "      <td>-0.327269</td>\n",
       "      <td>-0.113020</td>\n",
       "      <td>-0.158831</td>\n",
       "      <td>-0.173408</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.181686</td>\n",
       "      <td>-0.128897</td>\n",
       "      <td>-0.193744</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.128032</td>\n",
       "      <td>-0.127016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.237680</td>\n",
       "      <td>-0.283885</td>\n",
       "      <td>-0.262749</td>\n",
       "      <td>-0.237539</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.240506</td>\n",
       "      <td>-0.195062</td>\n",
       "      <td>-0.219176</td>\n",
       "      <td>-0.170003</td>\n",
       "      <td>-0.327269</td>\n",
       "      <td>-0.112947</td>\n",
       "      <td>-0.158831</td>\n",
       "      <td>-0.173323</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.181631</td>\n",
       "      <td>-0.128968</td>\n",
       "      <td>-0.193653</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.127868</td>\n",
       "      <td>-0.127016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.205081</td>\n",
       "      <td>-0.257032</td>\n",
       "      <td>-0.221823</td>\n",
       "      <td>-0.210507</td>\n",
       "      <td>-0.196147</td>\n",
       "      <td>-0.212986</td>\n",
       "      <td>-0.202461</td>\n",
       "      <td>-0.184733</td>\n",
       "      <td>-0.146118</td>\n",
       "      <td>-0.242611</td>\n",
       "      <td>-0.104094</td>\n",
       "      <td>-0.150289</td>\n",
       "      <td>-0.155821</td>\n",
       "      <td>-0.093080</td>\n",
       "      <td>-0.171468</td>\n",
       "      <td>-0.120538</td>\n",
       "      <td>-0.180182</td>\n",
       "      <td>-0.123729</td>\n",
       "      <td>-0.118418</td>\n",
       "      <td>-0.117917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.161295</td>\n",
       "      <td>7.610784</td>\n",
       "      <td>4.325499</td>\n",
       "      <td>5.389036</td>\n",
       "      <td>11.271672</td>\n",
       "      <td>1.678158</td>\n",
       "      <td>2.977458</td>\n",
       "      <td>0.659782</td>\n",
       "      <td>4.251222</td>\n",
       "      <td>1.553357</td>\n",
       "      <td>5.939371</td>\n",
       "      <td>4.620631</td>\n",
       "      <td>17.209243</td>\n",
       "      <td>5.795145</td>\n",
       "      <td>23.720365</td>\n",
       "      <td>8.329439</td>\n",
       "      <td>25.973607</td>\n",
       "      <td>15.287847</td>\n",
       "      <td>13.258032</td>\n",
       "      <td>8.344457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20497</th>\n",
       "      <td>-0.237936</td>\n",
       "      <td>-0.278151</td>\n",
       "      <td>-0.255565</td>\n",
       "      <td>-0.221896</td>\n",
       "      <td>-0.215083</td>\n",
       "      <td>-0.222839</td>\n",
       "      <td>-0.215396</td>\n",
       "      <td>-0.216464</td>\n",
       "      <td>-0.164038</td>\n",
       "      <td>-0.234033</td>\n",
       "      <td>-0.113020</td>\n",
       "      <td>-0.158623</td>\n",
       "      <td>-0.173323</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.181631</td>\n",
       "      <td>-0.128616</td>\n",
       "      <td>-0.193653</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.128032</td>\n",
       "      <td>-0.126883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20498</th>\n",
       "      <td>0.027078</td>\n",
       "      <td>-0.042573</td>\n",
       "      <td>-0.134075</td>\n",
       "      <td>-0.018481</td>\n",
       "      <td>-0.031766</td>\n",
       "      <td>-0.108912</td>\n",
       "      <td>-0.077528</td>\n",
       "      <td>-0.168777</td>\n",
       "      <td>-0.056130</td>\n",
       "      <td>-0.021716</td>\n",
       "      <td>-0.021378</td>\n",
       "      <td>-0.047679</td>\n",
       "      <td>-0.033593</td>\n",
       "      <td>-0.027739</td>\n",
       "      <td>-0.004701</td>\n",
       "      <td>-0.026959</td>\n",
       "      <td>-0.007284</td>\n",
       "      <td>-0.036701</td>\n",
       "      <td>-0.008642</td>\n",
       "      <td>-0.041510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20499</th>\n",
       "      <td>0.453567</td>\n",
       "      <td>0.437581</td>\n",
       "      <td>1.209685</td>\n",
       "      <td>1.395503</td>\n",
       "      <td>0.577594</td>\n",
       "      <td>0.390636</td>\n",
       "      <td>0.969745</td>\n",
       "      <td>0.724169</td>\n",
       "      <td>0.130132</td>\n",
       "      <td>1.101128</td>\n",
       "      <td>0.687686</td>\n",
       "      <td>1.011803</td>\n",
       "      <td>1.250106</td>\n",
       "      <td>0.438208</td>\n",
       "      <td>2.004382</td>\n",
       "      <td>0.868224</td>\n",
       "      <td>1.729439</td>\n",
       "      <td>1.635334</td>\n",
       "      <td>0.923658</td>\n",
       "      <td>1.121414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20500</th>\n",
       "      <td>-0.025755</td>\n",
       "      <td>0.065084</td>\n",
       "      <td>0.054222</td>\n",
       "      <td>0.044495</td>\n",
       "      <td>0.196145</td>\n",
       "      <td>-0.073413</td>\n",
       "      <td>-0.040898</td>\n",
       "      <td>-0.130096</td>\n",
       "      <td>0.093666</td>\n",
       "      <td>-0.143986</td>\n",
       "      <td>-0.025048</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>-0.085424</td>\n",
       "      <td>-0.005774</td>\n",
       "      <td>0.108933</td>\n",
       "      <td>0.086321</td>\n",
       "      <td>0.145991</td>\n",
       "      <td>0.056075</td>\n",
       "      <td>0.068434</td>\n",
       "      <td>0.100185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20501</th>\n",
       "      <td>-0.097900</td>\n",
       "      <td>-0.072428</td>\n",
       "      <td>-0.118451</td>\n",
       "      <td>-0.077416</td>\n",
       "      <td>-0.044985</td>\n",
       "      <td>-0.055250</td>\n",
       "      <td>-0.106227</td>\n",
       "      <td>-0.066906</td>\n",
       "      <td>-0.011937</td>\n",
       "      <td>-0.036192</td>\n",
       "      <td>-0.068062</td>\n",
       "      <td>-0.100705</td>\n",
       "      <td>-0.094848</td>\n",
       "      <td>-0.024678</td>\n",
       "      <td>-0.085475</td>\n",
       "      <td>-0.068171</td>\n",
       "      <td>-0.089972</td>\n",
       "      <td>-0.084648</td>\n",
       "      <td>-0.076894</td>\n",
       "      <td>-0.076024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20502 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sample0   Sample1   Sample2   Sample3    Sample4   Sample5   Sample6  \\\n",
       "0     -0.231762 -0.247491 -0.248976 -0.202346  -0.186685 -0.223989 -0.179934   \n",
       "1     -0.238106 -0.284280 -0.262749 -0.237539  -0.216971 -0.231590 -0.217126   \n",
       "2     -0.237680 -0.283885 -0.262749 -0.237539  -0.216971 -0.240506 -0.195062   \n",
       "3     -0.205081 -0.257032 -0.221823 -0.210507  -0.196147 -0.212986 -0.202461   \n",
       "4      2.161295  7.610784  4.325499  5.389036  11.271672  1.678158  2.977458   \n",
       "...         ...       ...       ...       ...        ...       ...       ...   \n",
       "20497 -0.237936 -0.278151 -0.255565 -0.221896  -0.215083 -0.222839 -0.215396   \n",
       "20498  0.027078 -0.042573 -0.134075 -0.018481  -0.031766 -0.108912 -0.077528   \n",
       "20499  0.453567  0.437581  1.209685  1.395503   0.577594  0.390636  0.969745   \n",
       "20500 -0.025755  0.065084  0.054222  0.044495   0.196145 -0.073413 -0.040898   \n",
       "20501 -0.097900 -0.072428 -0.118451 -0.077416  -0.044985 -0.055250 -0.106227   \n",
       "\n",
       "        Sample7   Sample8   Sample9  Sample10  Sample11   Sample12  Sample13  \\\n",
       "0     -0.214987 -0.166885 -0.280402 -0.089822 -0.124037  -0.114895 -0.098585   \n",
       "1     -0.219176 -0.167156 -0.327269 -0.113020 -0.158831  -0.173408 -0.107497   \n",
       "2     -0.219176 -0.170003 -0.327269 -0.112947 -0.158831  -0.173323 -0.107497   \n",
       "3     -0.184733 -0.146118 -0.242611 -0.104094 -0.150289  -0.155821 -0.093080   \n",
       "4      0.659782  4.251222  1.553357  5.939371  4.620631  17.209243  5.795145   \n",
       "...         ...       ...       ...       ...       ...        ...       ...   \n",
       "20497 -0.216464 -0.164038 -0.234033 -0.113020 -0.158623  -0.173323 -0.107497   \n",
       "20498 -0.168777 -0.056130 -0.021716 -0.021378 -0.047679  -0.033593 -0.027739   \n",
       "20499  0.724169  0.130132  1.101128  0.687686  1.011803   1.250106  0.438208   \n",
       "20500 -0.130096  0.093666 -0.143986 -0.025048  0.000924  -0.085424 -0.005774   \n",
       "20501 -0.066906 -0.011937 -0.036192 -0.068062 -0.100705  -0.094848 -0.024678   \n",
       "\n",
       "        Sample14  Sample15   Sample16   Sample17   Sample18  Sample19  \n",
       "0      -0.166281 -0.111795  -0.179398  -0.124769  -0.115809 -0.115490  \n",
       "1      -0.181686 -0.128897  -0.193744  -0.130680  -0.128032 -0.127016  \n",
       "2      -0.181631 -0.128968  -0.193653  -0.130680  -0.127868 -0.127016  \n",
       "3      -0.171468 -0.120538  -0.180182  -0.123729  -0.118418 -0.117917  \n",
       "4      23.720365  8.329439  25.973607  15.287847  13.258032  8.344457  \n",
       "...          ...       ...        ...        ...        ...       ...  \n",
       "20497  -0.181631 -0.128616  -0.193653  -0.130680  -0.128032 -0.126883  \n",
       "20498  -0.004701 -0.026959  -0.007284  -0.036701  -0.008642 -0.041510  \n",
       "20499   2.004382  0.868224   1.729439   1.635334   0.923658  1.121414  \n",
       "20500   0.108933  0.086321   0.145991   0.056075   0.068434  0.100185  \n",
       "20501  -0.085475 -0.068171  -0.089972  -0.084648  -0.076894 -0.076024  \n",
       "\n",
       "[20502 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#z-scoring\n",
    "df_data = StandByCol(df_data,unused_cols=[])\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20492</th>\n",
       "      <th>20493</th>\n",
       "      <th>20494</th>\n",
       "      <th>20495</th>\n",
       "      <th>20496</th>\n",
       "      <th>20497</th>\n",
       "      <th>20498</th>\n",
       "      <th>20499</th>\n",
       "      <th>20500</th>\n",
       "      <th>20501</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sample0</th>\n",
       "      <td>-0.231762</td>\n",
       "      <td>-0.238106</td>\n",
       "      <td>-0.237680</td>\n",
       "      <td>-0.205081</td>\n",
       "      <td>2.161295</td>\n",
       "      <td>-0.238021</td>\n",
       "      <td>-0.206202</td>\n",
       "      <td>-0.237850</td>\n",
       "      <td>-0.238106</td>\n",
       "      <td>-0.079438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184203</td>\n",
       "      <td>-0.111937</td>\n",
       "      <td>-0.228407</td>\n",
       "      <td>-0.166812</td>\n",
       "      <td>-0.004315</td>\n",
       "      <td>-0.237936</td>\n",
       "      <td>0.027078</td>\n",
       "      <td>0.453567</td>\n",
       "      <td>-0.025755</td>\n",
       "      <td>-0.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample1</th>\n",
       "      <td>-0.247491</td>\n",
       "      <td>-0.284280</td>\n",
       "      <td>-0.283885</td>\n",
       "      <td>-0.257032</td>\n",
       "      <td>7.610784</td>\n",
       "      <td>-0.283885</td>\n",
       "      <td>-0.106731</td>\n",
       "      <td>-0.284181</td>\n",
       "      <td>-0.284280</td>\n",
       "      <td>-0.098823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223402</td>\n",
       "      <td>-0.196099</td>\n",
       "      <td>-0.265300</td>\n",
       "      <td>-0.132830</td>\n",
       "      <td>0.077738</td>\n",
       "      <td>-0.278151</td>\n",
       "      <td>-0.042573</td>\n",
       "      <td>0.437581</td>\n",
       "      <td>0.065084</td>\n",
       "      <td>-0.072428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample2</th>\n",
       "      <td>-0.248976</td>\n",
       "      <td>-0.262749</td>\n",
       "      <td>-0.262749</td>\n",
       "      <td>-0.221823</td>\n",
       "      <td>4.325499</td>\n",
       "      <td>-0.262749</td>\n",
       "      <td>-0.166221</td>\n",
       "      <td>-0.262479</td>\n",
       "      <td>-0.262390</td>\n",
       "      <td>-0.103365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121692</td>\n",
       "      <td>0.020729</td>\n",
       "      <td>-0.246496</td>\n",
       "      <td>-0.163796</td>\n",
       "      <td>-0.034224</td>\n",
       "      <td>-0.255565</td>\n",
       "      <td>-0.134075</td>\n",
       "      <td>1.209685</td>\n",
       "      <td>0.054222</td>\n",
       "      <td>-0.118451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample3</th>\n",
       "      <td>-0.202346</td>\n",
       "      <td>-0.237539</td>\n",
       "      <td>-0.237539</td>\n",
       "      <td>-0.210507</td>\n",
       "      <td>5.389036</td>\n",
       "      <td>-0.236385</td>\n",
       "      <td>-0.103795</td>\n",
       "      <td>-0.237020</td>\n",
       "      <td>-0.237193</td>\n",
       "      <td>-0.090807</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133908</td>\n",
       "      <td>-0.055885</td>\n",
       "      <td>-0.227784</td>\n",
       "      <td>-0.153899</td>\n",
       "      <td>-0.009187</td>\n",
       "      <td>-0.221896</td>\n",
       "      <td>-0.018481</td>\n",
       "      <td>1.395503</td>\n",
       "      <td>0.044495</td>\n",
       "      <td>-0.077416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample4</th>\n",
       "      <td>-0.186685</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.196147</td>\n",
       "      <td>11.271672</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.180656</td>\n",
       "      <td>-0.216535</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.058639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155550</td>\n",
       "      <td>-0.180366</td>\n",
       "      <td>-0.197942</td>\n",
       "      <td>-0.083043</td>\n",
       "      <td>0.097078</td>\n",
       "      <td>-0.215083</td>\n",
       "      <td>-0.031766</td>\n",
       "      <td>0.577594</td>\n",
       "      <td>0.196145</td>\n",
       "      <td>-0.044985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample5</th>\n",
       "      <td>-0.223989</td>\n",
       "      <td>-0.231590</td>\n",
       "      <td>-0.240506</td>\n",
       "      <td>-0.212986</td>\n",
       "      <td>1.678158</td>\n",
       "      <td>-0.231508</td>\n",
       "      <td>-0.142264</td>\n",
       "      <td>-0.239928</td>\n",
       "      <td>-0.240424</td>\n",
       "      <td>-0.064497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138837</td>\n",
       "      <td>-0.021732</td>\n",
       "      <td>-0.220941</td>\n",
       "      <td>-0.086622</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>-0.222839</td>\n",
       "      <td>-0.108912</td>\n",
       "      <td>0.390636</td>\n",
       "      <td>-0.073413</td>\n",
       "      <td>-0.055250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample6</th>\n",
       "      <td>-0.179934</td>\n",
       "      <td>-0.217126</td>\n",
       "      <td>-0.195062</td>\n",
       "      <td>-0.202461</td>\n",
       "      <td>2.977458</td>\n",
       "      <td>-0.216982</td>\n",
       "      <td>-0.068587</td>\n",
       "      <td>-0.216982</td>\n",
       "      <td>-0.215828</td>\n",
       "      <td>-0.056906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143433</td>\n",
       "      <td>0.109083</td>\n",
       "      <td>-0.206166</td>\n",
       "      <td>-0.056041</td>\n",
       "      <td>-0.039889</td>\n",
       "      <td>-0.215396</td>\n",
       "      <td>-0.077528</td>\n",
       "      <td>0.969745</td>\n",
       "      <td>-0.040898</td>\n",
       "      <td>-0.106227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample7</th>\n",
       "      <td>-0.214987</td>\n",
       "      <td>-0.219176</td>\n",
       "      <td>-0.219176</td>\n",
       "      <td>-0.184733</td>\n",
       "      <td>0.659782</td>\n",
       "      <td>-0.144678</td>\n",
       "      <td>-0.100471</td>\n",
       "      <td>-0.219073</td>\n",
       "      <td>-0.219176</td>\n",
       "      <td>-0.047668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150555</td>\n",
       "      <td>-0.089982</td>\n",
       "      <td>-0.216106</td>\n",
       "      <td>-0.181722</td>\n",
       "      <td>-0.076986</td>\n",
       "      <td>-0.216464</td>\n",
       "      <td>-0.168777</td>\n",
       "      <td>0.724169</td>\n",
       "      <td>-0.130096</td>\n",
       "      <td>-0.066906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample8</th>\n",
       "      <td>-0.166885</td>\n",
       "      <td>-0.167156</td>\n",
       "      <td>-0.170003</td>\n",
       "      <td>-0.146118</td>\n",
       "      <td>4.251222</td>\n",
       "      <td>-0.169867</td>\n",
       "      <td>-0.132317</td>\n",
       "      <td>-0.169867</td>\n",
       "      <td>-0.169867</td>\n",
       "      <td>-0.063451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076218</td>\n",
       "      <td>-0.101544</td>\n",
       "      <td>-0.155091</td>\n",
       "      <td>-0.057622</td>\n",
       "      <td>0.114271</td>\n",
       "      <td>-0.164038</td>\n",
       "      <td>-0.056130</td>\n",
       "      <td>0.130132</td>\n",
       "      <td>0.093666</td>\n",
       "      <td>-0.011937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample9</th>\n",
       "      <td>-0.280402</td>\n",
       "      <td>-0.327269</td>\n",
       "      <td>-0.327269</td>\n",
       "      <td>-0.242611</td>\n",
       "      <td>1.553357</td>\n",
       "      <td>-0.320154</td>\n",
       "      <td>-0.015009</td>\n",
       "      <td>-0.325306</td>\n",
       "      <td>-0.327269</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032596</td>\n",
       "      <td>0.130652</td>\n",
       "      <td>-0.305841</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>0.112169</td>\n",
       "      <td>-0.234033</td>\n",
       "      <td>-0.021716</td>\n",
       "      <td>1.101128</td>\n",
       "      <td>-0.143986</td>\n",
       "      <td>-0.036192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample10</th>\n",
       "      <td>-0.089822</td>\n",
       "      <td>-0.113020</td>\n",
       "      <td>-0.112947</td>\n",
       "      <td>-0.104094</td>\n",
       "      <td>5.939371</td>\n",
       "      <td>-0.113057</td>\n",
       "      <td>-0.053858</td>\n",
       "      <td>-0.112617</td>\n",
       "      <td>-0.113057</td>\n",
       "      <td>-0.053051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102086</td>\n",
       "      <td>-0.106855</td>\n",
       "      <td>-0.108579</td>\n",
       "      <td>-0.084210</td>\n",
       "      <td>-0.020093</td>\n",
       "      <td>-0.113020</td>\n",
       "      <td>-0.021378</td>\n",
       "      <td>0.687686</td>\n",
       "      <td>-0.025048</td>\n",
       "      <td>-0.068062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample11</th>\n",
       "      <td>-0.124037</td>\n",
       "      <td>-0.158831</td>\n",
       "      <td>-0.158831</td>\n",
       "      <td>-0.150289</td>\n",
       "      <td>4.620631</td>\n",
       "      <td>-0.158779</td>\n",
       "      <td>-0.079526</td>\n",
       "      <td>-0.158363</td>\n",
       "      <td>-0.158831</td>\n",
       "      <td>-0.083221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140674</td>\n",
       "      <td>-0.140098</td>\n",
       "      <td>-0.154096</td>\n",
       "      <td>-0.117670</td>\n",
       "      <td>-0.028790</td>\n",
       "      <td>-0.158623</td>\n",
       "      <td>-0.047679</td>\n",
       "      <td>1.011803</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>-0.100705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample12</th>\n",
       "      <td>-0.114895</td>\n",
       "      <td>-0.173408</td>\n",
       "      <td>-0.173323</td>\n",
       "      <td>-0.155821</td>\n",
       "      <td>17.209243</td>\n",
       "      <td>-0.173408</td>\n",
       "      <td>-0.106242</td>\n",
       "      <td>-0.172809</td>\n",
       "      <td>-0.173408</td>\n",
       "      <td>-0.086795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145394</td>\n",
       "      <td>-0.145308</td>\n",
       "      <td>-0.167411</td>\n",
       "      <td>-0.130658</td>\n",
       "      <td>-0.079170</td>\n",
       "      <td>-0.173323</td>\n",
       "      <td>-0.033593</td>\n",
       "      <td>1.250106</td>\n",
       "      <td>-0.085424</td>\n",
       "      <td>-0.094848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample13</th>\n",
       "      <td>-0.098585</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.093080</td>\n",
       "      <td>5.795145</td>\n",
       "      <td>-0.107227</td>\n",
       "      <td>-0.090213</td>\n",
       "      <td>-0.107407</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.063747</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087062</td>\n",
       "      <td>-0.094264</td>\n",
       "      <td>-0.103086</td>\n",
       "      <td>-0.080581</td>\n",
       "      <td>-0.029899</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.027739</td>\n",
       "      <td>0.438208</td>\n",
       "      <td>-0.005774</td>\n",
       "      <td>-0.024678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample14</th>\n",
       "      <td>-0.166281</td>\n",
       "      <td>-0.181686</td>\n",
       "      <td>-0.181631</td>\n",
       "      <td>-0.171468</td>\n",
       "      <td>23.720365</td>\n",
       "      <td>-0.181576</td>\n",
       "      <td>-0.058183</td>\n",
       "      <td>-0.181190</td>\n",
       "      <td>-0.181686</td>\n",
       "      <td>-0.083159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156549</td>\n",
       "      <td>-0.169005</td>\n",
       "      <td>-0.168068</td>\n",
       "      <td>-0.103614</td>\n",
       "      <td>0.016746</td>\n",
       "      <td>-0.181631</td>\n",
       "      <td>-0.004701</td>\n",
       "      <td>2.004382</td>\n",
       "      <td>0.108933</td>\n",
       "      <td>-0.085475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample15</th>\n",
       "      <td>-0.111795</td>\n",
       "      <td>-0.128897</td>\n",
       "      <td>-0.128968</td>\n",
       "      <td>-0.120538</td>\n",
       "      <td>8.329439</td>\n",
       "      <td>-0.122768</td>\n",
       "      <td>-0.066974</td>\n",
       "      <td>-0.128756</td>\n",
       "      <td>-0.128968</td>\n",
       "      <td>-0.053588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105659</td>\n",
       "      <td>-0.090855</td>\n",
       "      <td>-0.121782</td>\n",
       "      <td>-0.087051</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>-0.128616</td>\n",
       "      <td>-0.026959</td>\n",
       "      <td>0.868224</td>\n",
       "      <td>0.086321</td>\n",
       "      <td>-0.068171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample16</th>\n",
       "      <td>-0.179398</td>\n",
       "      <td>-0.193744</td>\n",
       "      <td>-0.193653</td>\n",
       "      <td>-0.180182</td>\n",
       "      <td>25.973607</td>\n",
       "      <td>-0.193744</td>\n",
       "      <td>-0.098497</td>\n",
       "      <td>-0.193378</td>\n",
       "      <td>-0.193744</td>\n",
       "      <td>-0.086855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162301</td>\n",
       "      <td>-0.171835</td>\n",
       "      <td>-0.182010</td>\n",
       "      <td>-0.124349</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>-0.193653</td>\n",
       "      <td>-0.007284</td>\n",
       "      <td>1.729439</td>\n",
       "      <td>0.145991</td>\n",
       "      <td>-0.089972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample17</th>\n",
       "      <td>-0.124769</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.123729</td>\n",
       "      <td>15.287847</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.062481</td>\n",
       "      <td>-0.130188</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.066586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115847</td>\n",
       "      <td>-0.123017</td>\n",
       "      <td>-0.125152</td>\n",
       "      <td>-0.077807</td>\n",
       "      <td>-0.003641</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.036701</td>\n",
       "      <td>1.635334</td>\n",
       "      <td>0.056075</td>\n",
       "      <td>-0.084648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample18</th>\n",
       "      <td>-0.115809</td>\n",
       "      <td>-0.128032</td>\n",
       "      <td>-0.127868</td>\n",
       "      <td>-0.118418</td>\n",
       "      <td>13.258032</td>\n",
       "      <td>-0.127909</td>\n",
       "      <td>-0.052269</td>\n",
       "      <td>-0.127868</td>\n",
       "      <td>-0.128032</td>\n",
       "      <td>-0.060765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108138</td>\n",
       "      <td>-0.120029</td>\n",
       "      <td>-0.122820</td>\n",
       "      <td>-0.092818</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>-0.128032</td>\n",
       "      <td>-0.008642</td>\n",
       "      <td>0.923658</td>\n",
       "      <td>0.068434</td>\n",
       "      <td>-0.076894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample19</th>\n",
       "      <td>-0.115490</td>\n",
       "      <td>-0.127016</td>\n",
       "      <td>-0.127016</td>\n",
       "      <td>-0.117917</td>\n",
       "      <td>8.344457</td>\n",
       "      <td>-0.126927</td>\n",
       "      <td>-0.053948</td>\n",
       "      <td>-0.126883</td>\n",
       "      <td>-0.127016</td>\n",
       "      <td>-0.072159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108272</td>\n",
       "      <td>-0.103608</td>\n",
       "      <td>-0.122352</td>\n",
       "      <td>-0.084374</td>\n",
       "      <td>-0.017347</td>\n",
       "      <td>-0.126883</td>\n",
       "      <td>-0.041510</td>\n",
       "      <td>1.121414</td>\n",
       "      <td>0.100185</td>\n",
       "      <td>-0.076024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 20502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3          4         5      \\\n",
       "Sample0  -0.231762 -0.238106 -0.237680 -0.205081   2.161295 -0.238021   \n",
       "Sample1  -0.247491 -0.284280 -0.283885 -0.257032   7.610784 -0.283885   \n",
       "Sample2  -0.248976 -0.262749 -0.262749 -0.221823   4.325499 -0.262749   \n",
       "Sample3  -0.202346 -0.237539 -0.237539 -0.210507   5.389036 -0.236385   \n",
       "Sample4  -0.186685 -0.216971 -0.216971 -0.196147  11.271672 -0.216971   \n",
       "Sample5  -0.223989 -0.231590 -0.240506 -0.212986   1.678158 -0.231508   \n",
       "Sample6  -0.179934 -0.217126 -0.195062 -0.202461   2.977458 -0.216982   \n",
       "Sample7  -0.214987 -0.219176 -0.219176 -0.184733   0.659782 -0.144678   \n",
       "Sample8  -0.166885 -0.167156 -0.170003 -0.146118   4.251222 -0.169867   \n",
       "Sample9  -0.280402 -0.327269 -0.327269 -0.242611   1.553357 -0.320154   \n",
       "Sample10 -0.089822 -0.113020 -0.112947 -0.104094   5.939371 -0.113057   \n",
       "Sample11 -0.124037 -0.158831 -0.158831 -0.150289   4.620631 -0.158779   \n",
       "Sample12 -0.114895 -0.173408 -0.173323 -0.155821  17.209243 -0.173408   \n",
       "Sample13 -0.098585 -0.107497 -0.107497 -0.093080   5.795145 -0.107227   \n",
       "Sample14 -0.166281 -0.181686 -0.181631 -0.171468  23.720365 -0.181576   \n",
       "Sample15 -0.111795 -0.128897 -0.128968 -0.120538   8.329439 -0.122768   \n",
       "Sample16 -0.179398 -0.193744 -0.193653 -0.180182  25.973607 -0.193744   \n",
       "Sample17 -0.124769 -0.130680 -0.130680 -0.123729  15.287847 -0.130680   \n",
       "Sample18 -0.115809 -0.128032 -0.127868 -0.118418  13.258032 -0.127909   \n",
       "Sample19 -0.115490 -0.127016 -0.127016 -0.117917   8.344457 -0.126927   \n",
       "\n",
       "             6         7         8         9      ...     20492     20493  \\\n",
       "Sample0  -0.206202 -0.237850 -0.238106 -0.079438  ... -0.184203 -0.111937   \n",
       "Sample1  -0.106731 -0.284181 -0.284280 -0.098823  ... -0.223402 -0.196099   \n",
       "Sample2  -0.166221 -0.262479 -0.262390 -0.103365  ... -0.121692  0.020729   \n",
       "Sample3  -0.103795 -0.237020 -0.237193 -0.090807  ... -0.133908 -0.055885   \n",
       "Sample4  -0.180656 -0.216535 -0.216971 -0.058639  ... -0.155550 -0.180366   \n",
       "Sample5  -0.142264 -0.239928 -0.240424 -0.064497  ... -0.138837 -0.021732   \n",
       "Sample6  -0.068587 -0.216982 -0.215828 -0.056906  ... -0.143433  0.109083   \n",
       "Sample7  -0.100471 -0.219073 -0.219176 -0.047668  ... -0.150555 -0.089982   \n",
       "Sample8  -0.132317 -0.169867 -0.169867 -0.063451  ... -0.076218 -0.101544   \n",
       "Sample9  -0.015009 -0.325306 -0.327269 -0.002087  ... -0.032596  0.130652   \n",
       "Sample10 -0.053858 -0.112617 -0.113057 -0.053051  ... -0.102086 -0.106855   \n",
       "Sample11 -0.079526 -0.158363 -0.158831 -0.083221  ... -0.140674 -0.140098   \n",
       "Sample12 -0.106242 -0.172809 -0.173408 -0.086795  ... -0.145394 -0.145308   \n",
       "Sample13 -0.090213 -0.107407 -0.107497 -0.063747  ... -0.087062 -0.094264   \n",
       "Sample14 -0.058183 -0.181190 -0.181686 -0.083159  ... -0.156549 -0.169005   \n",
       "Sample15 -0.066974 -0.128756 -0.128968 -0.053588  ... -0.105659 -0.090855   \n",
       "Sample16 -0.098497 -0.193378 -0.193744 -0.086855  ... -0.162301 -0.171835   \n",
       "Sample17 -0.062481 -0.130188 -0.130680 -0.066586  ... -0.115847 -0.123017   \n",
       "Sample18 -0.052269 -0.127868 -0.128032 -0.060765  ... -0.108138 -0.120029   \n",
       "Sample19 -0.053948 -0.126883 -0.127016 -0.072159  ... -0.108272 -0.103608   \n",
       "\n",
       "             20494     20495     20496     20497     20498     20499  \\\n",
       "Sample0  -0.228407 -0.166812 -0.004315 -0.237936  0.027078  0.453567   \n",
       "Sample1  -0.265300 -0.132830  0.077738 -0.278151 -0.042573  0.437581   \n",
       "Sample2  -0.246496 -0.163796 -0.034224 -0.255565 -0.134075  1.209685   \n",
       "Sample3  -0.227784 -0.153899 -0.009187 -0.221896 -0.018481  1.395503   \n",
       "Sample4  -0.197942 -0.083043  0.097078 -0.215083 -0.031766  0.577594   \n",
       "Sample5  -0.220941 -0.086622  0.008318 -0.222839 -0.108912  0.390636   \n",
       "Sample6  -0.206166 -0.056041 -0.039889 -0.215396 -0.077528  0.969745   \n",
       "Sample7  -0.216106 -0.181722 -0.076986 -0.216464 -0.168777  0.724169   \n",
       "Sample8  -0.155091 -0.057622  0.114271 -0.164038 -0.056130  0.130132   \n",
       "Sample9  -0.305841 -0.196902  0.112169 -0.234033 -0.021716  1.101128   \n",
       "Sample10 -0.108579 -0.084210 -0.020093 -0.113020 -0.021378  0.687686   \n",
       "Sample11 -0.154096 -0.117670 -0.028790 -0.158623 -0.047679  1.011803   \n",
       "Sample12 -0.167411 -0.130658 -0.079170 -0.173323 -0.033593  1.250106   \n",
       "Sample13 -0.103086 -0.080581 -0.029899 -0.107497 -0.027739  0.438208   \n",
       "Sample14 -0.168068 -0.103614  0.016746 -0.181631 -0.004701  2.004382   \n",
       "Sample15 -0.121782 -0.087051 -0.001034 -0.128616 -0.026959  0.868224   \n",
       "Sample16 -0.182010 -0.124349  0.003258 -0.193653 -0.007284  1.729439   \n",
       "Sample17 -0.125152 -0.077807 -0.003641 -0.130680 -0.036701  1.635334   \n",
       "Sample18 -0.122820 -0.092818  0.016352 -0.128032 -0.008642  0.923658   \n",
       "Sample19 -0.122352 -0.084374 -0.017347 -0.126883 -0.041510  1.121414   \n",
       "\n",
       "             20500     20501  \n",
       "Sample0  -0.025755 -0.097900  \n",
       "Sample1   0.065084 -0.072428  \n",
       "Sample2   0.054222 -0.118451  \n",
       "Sample3   0.044495 -0.077416  \n",
       "Sample4   0.196145 -0.044985  \n",
       "Sample5  -0.073413 -0.055250  \n",
       "Sample6  -0.040898 -0.106227  \n",
       "Sample7  -0.130096 -0.066906  \n",
       "Sample8   0.093666 -0.011937  \n",
       "Sample9  -0.143986 -0.036192  \n",
       "Sample10 -0.025048 -0.068062  \n",
       "Sample11  0.000924 -0.100705  \n",
       "Sample12 -0.085424 -0.094848  \n",
       "Sample13 -0.005774 -0.024678  \n",
       "Sample14  0.108933 -0.085475  \n",
       "Sample15  0.086321 -0.068171  \n",
       "Sample16  0.145991 -0.089972  \n",
       "Sample17  0.056075 -0.084648  \n",
       "Sample18  0.068434 -0.076894  \n",
       "Sample19  0.100185 -0.076024  \n",
       "\n",
       "[20 rows x 20502 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = df_data.T\n",
    "x_all = df_data.values  \n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno = pd.read_csv('input_data_2.csv')\n",
    "anno_ids = df_anno.id.values\n",
    "anno_class = df_anno.label.values\n",
    "class_type = df_anno.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sample0', 'Sample1', 'Sample2', 'Sample3', 'Sample4', 'Sample5',\n",
       "       'Sample6', 'Sample7', 'Sample8', 'Sample9', 'Sample10', 'Sample11',\n",
       "       'Sample12', 'Sample13', 'Sample14', 'Sample15', 'Sample16',\n",
       "       'Sample17', 'Sample18', 'Sample19'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Tumor', 'Tumor', 'Tumor', 'Tumor', 'Tumor', 'Tumor', 'Tumor',\n",
       "       'Tumor', 'Tumor', 'Tumor', 'Normal', 'Normal', 'Normal', 'Normal',\n",
       "       'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Tumor', 'Normal'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cls = len(class_type)\n",
    "y_all = []\n",
    "for id_ in id_names:\n",
    "    idx = np.where(anno_ids == id_)\n",
    "    y_all.append(np.where(class_type == anno_class[idx])[0][0])\n",
    "y_all = np.asarray(y_all)\n",
    "y_all = to_categorical(y_all, num_classes=n_cls)\n",
    "\n",
    "y_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to train and val\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_all, y_all, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.48976301e-01, -2.62748842e-01, -2.62748842e-01, ...,\n",
       "         1.20968516e+00,  5.42222426e-02, -1.18450672e-01],\n",
       "       [-1.66281463e-01, -1.81686327e-01, -1.81631186e-01, ...,\n",
       "         2.00438175e+00,  1.08932969e-01, -8.54748492e-02],\n",
       "       [-1.86684604e-01, -2.16971028e-01, -2.16971028e-01, ...,\n",
       "         5.77593876e-01,  1.96144627e-01, -4.49847619e-02],\n",
       "       ...,\n",
       "       [-1.14894803e-01, -1.73408296e-01, -1.73322625e-01, ...,\n",
       "         1.25010571e+00, -8.54238940e-02, -9.48477255e-02],\n",
       "       [-1.24037335e-01, -1.58831134e-01, -1.58831134e-01, ...,\n",
       "         1.01180311e+00,  9.23805117e-04, -1.00705319e-01],\n",
       "       [-2.23988538e-01, -2.31590305e-01, -2.40506371e-01, ...,\n",
       "         3.90635687e-01, -7.34126509e-02, -5.52502900e-02]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = WxHyperParameter(epochs=30, learning_ratio=0.01, batch_size=8, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0491 - accuracy: 0.6250\n",
      "Epoch 00001: val_loss improved from inf to 1.79270, saving model to ./slp_wx_weights_best.hdf5\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 1.6983 - accuracy: 0.5000 - val_loss: 1.7927 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7792 - accuracy: 0.5000\n",
      "Epoch 00002: val_loss improved from 1.79270 to 0.00000, saving model to ./slp_wx_weights_best.hdf5\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.4275 - accuracy: 0.7500 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1332e-04 - accuracy: 1.0000\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 5.6658e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.4703e-08 - accuracy: 1.0000\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 2.2352e-08 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00005: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00006: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00007: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00008: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00009: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00010: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00011: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00012: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00013: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00014: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00015: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00016: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00017: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00018: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00019: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00020: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00021: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 22/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00022: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 23/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00023: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 24/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00024: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 25/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00025: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 26/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00026: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 27/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00027: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00028: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 29/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00029: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 30/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00030: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "1/1 [==============================] - 0s 767us/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "\n",
      "function [wx_slp] finished in 2.8156471252441406 s\n"
     ]
    }
   ],
   "source": [
    "sel_idx, sel_weight, val_acc = wx_slp(x_train, y_train, x_val, y_val, n_selection=10, hyper_param=hp, num_cls=n_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single Layer WX\n",
      "selected feature names: ['SFTPC' 'SFTPA2' 'SFTPA1' 'FTL' 'EEF1A1' 'ACTB' 'GAPDH' 'ACTG1' 'HLA-B'\n",
      " 'FGG']\n",
      "selected feature index: [15871 15869 15868  6668  5350   200  6827   203  7870  6380]\n",
      "selected feature weight: [27.63028513 18.62849216  5.74430402  3.84302923  2.83739353  2.40131759\n",
      "  1.24756945  1.1751258   0.95742483  0.81372882]\n",
      "evaluation accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print ('\\nSingle Layer WX')\n",
    "print ('selected feature names:',f_names[sel_idx])\n",
    "print ('selected feature index:',sel_idx)\n",
    "print ('selected feature weight:',sel_weight)\n",
    "print ('evaluation accuracy:',val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20502"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_genes = len(f_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4235 - accuracy: 0.7500\n",
      "Epoch 00001: val_loss improved from inf to 2.24856, saving model to ./slp_wx_weights_best.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1801 - accuracy: 0.5625 - val_loss: 2.2486 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.2524 - accuracy: 0.6250\n",
      "Epoch 00002: val_loss improved from 2.24856 to 0.00000, saving model to ./slp_wx_weights_best.hdf5\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.0208 - accuracy: 0.6875 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0916e-04 - accuracy: 1.0000\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.5458e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00005: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00006: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00007: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00008: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00009: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00010: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00011: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00012: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00013: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00014: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00015: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00016: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00017: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00018: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00019: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00020: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00021: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 22/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00022: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 23/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00023: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 24/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00024: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 25/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00025: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 26/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00026: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 27/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00027: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00028: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 29/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00029: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 30/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00030: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "1/1 [==============================] - 0s 689us/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "\n",
      "function [wx_slp] finished in 1.2418487071990967 s\n"
     ]
    }
   ],
   "source": [
    "sel_idx, sel_weight, val_acc = wx_slp(x_train, y_train, x_val, y_val, n_selection=len(f_names), hyper_param=hp, num_cls=n_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
