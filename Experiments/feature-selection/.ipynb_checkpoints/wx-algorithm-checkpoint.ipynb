{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from wx_hyperparam import WxHyperParameter\n",
    "from wx_core import wx_slp, wx_mlp, connection_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(num_cls=2):\n",
    "    train_num = 100\n",
    "    test_num = 100\n",
    "    input_dim = 20000\n",
    "    num_cls = num_cls\n",
    "    if num_cls < 2:\n",
    "        return\n",
    "\n",
    "    x_train = np.random.random((train_num, input_dim))\n",
    "    y_train = to_categorical(np.random.randint(num_cls, size=(train_num, 1)), num_classes=num_cls)\n",
    "\n",
    "    x_test = np.random.random((test_num, input_dim))\n",
    "    y_test = to_categorical(np.random.randint(num_cls, size=(test_num, 1)), num_classes=num_cls)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def StandByCol(data_frame, unused_cols=[]):\n",
    "    unused_cols_list = []\n",
    "    for col_ in unused_cols:\n",
    "        unused_cols_list.append(data_frame[col_])\n",
    "    data_frame = data_frame.drop(unused_cols,axis=1)\n",
    "\n",
    "    data_frame = data_frame.astype(float)\n",
    "    data_frame.fillna(0, inplace=True)\n",
    "    data_frame = data_frame.apply(lambda x: ((x-x.mean())/x.std()), axis=0)\n",
    "\n",
    "    for n,col_ in enumerate(unused_cols):\n",
    "        data_frame[col_] = unused_cols_list[n]\n",
    "\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from actual datas\n",
    "df_data = pd.read_csv('input_data_1.csv')\n",
    "f_names = df_data['fnames'].values\n",
    "df_data = df_data.drop(['fnames'],axis=1)\n",
    "id_names = df_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample0</th>\n",
       "      <th>Sample1</th>\n",
       "      <th>Sample2</th>\n",
       "      <th>Sample3</th>\n",
       "      <th>Sample4</th>\n",
       "      <th>Sample5</th>\n",
       "      <th>Sample6</th>\n",
       "      <th>Sample7</th>\n",
       "      <th>Sample8</th>\n",
       "      <th>Sample9</th>\n",
       "      <th>Sample10</th>\n",
       "      <th>Sample11</th>\n",
       "      <th>Sample12</th>\n",
       "      <th>Sample13</th>\n",
       "      <th>Sample14</th>\n",
       "      <th>Sample15</th>\n",
       "      <th>Sample16</th>\n",
       "      <th>Sample17</th>\n",
       "      <th>Sample18</th>\n",
       "      <th>Sample19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.231762</td>\n",
       "      <td>-0.247491</td>\n",
       "      <td>-0.248976</td>\n",
       "      <td>-0.202346</td>\n",
       "      <td>-0.186685</td>\n",
       "      <td>-0.223989</td>\n",
       "      <td>-0.179934</td>\n",
       "      <td>-0.214987</td>\n",
       "      <td>-0.166885</td>\n",
       "      <td>-0.280402</td>\n",
       "      <td>-0.089822</td>\n",
       "      <td>-0.124037</td>\n",
       "      <td>-0.114895</td>\n",
       "      <td>-0.098585</td>\n",
       "      <td>-0.166281</td>\n",
       "      <td>-0.111795</td>\n",
       "      <td>-0.179398</td>\n",
       "      <td>-0.124769</td>\n",
       "      <td>-0.115809</td>\n",
       "      <td>-0.115490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.238106</td>\n",
       "      <td>-0.284280</td>\n",
       "      <td>-0.262749</td>\n",
       "      <td>-0.237539</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.231590</td>\n",
       "      <td>-0.217126</td>\n",
       "      <td>-0.219176</td>\n",
       "      <td>-0.167156</td>\n",
       "      <td>-0.327269</td>\n",
       "      <td>-0.113020</td>\n",
       "      <td>-0.158831</td>\n",
       "      <td>-0.173408</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.181686</td>\n",
       "      <td>-0.128897</td>\n",
       "      <td>-0.193744</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.128032</td>\n",
       "      <td>-0.127016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.237680</td>\n",
       "      <td>-0.283885</td>\n",
       "      <td>-0.262749</td>\n",
       "      <td>-0.237539</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.240506</td>\n",
       "      <td>-0.195062</td>\n",
       "      <td>-0.219176</td>\n",
       "      <td>-0.170003</td>\n",
       "      <td>-0.327269</td>\n",
       "      <td>-0.112947</td>\n",
       "      <td>-0.158831</td>\n",
       "      <td>-0.173323</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.181631</td>\n",
       "      <td>-0.128968</td>\n",
       "      <td>-0.193653</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.127868</td>\n",
       "      <td>-0.127016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.205081</td>\n",
       "      <td>-0.257032</td>\n",
       "      <td>-0.221823</td>\n",
       "      <td>-0.210507</td>\n",
       "      <td>-0.196147</td>\n",
       "      <td>-0.212986</td>\n",
       "      <td>-0.202461</td>\n",
       "      <td>-0.184733</td>\n",
       "      <td>-0.146118</td>\n",
       "      <td>-0.242611</td>\n",
       "      <td>-0.104094</td>\n",
       "      <td>-0.150289</td>\n",
       "      <td>-0.155821</td>\n",
       "      <td>-0.093080</td>\n",
       "      <td>-0.171468</td>\n",
       "      <td>-0.120538</td>\n",
       "      <td>-0.180182</td>\n",
       "      <td>-0.123729</td>\n",
       "      <td>-0.118418</td>\n",
       "      <td>-0.117917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.161295</td>\n",
       "      <td>7.610784</td>\n",
       "      <td>4.325499</td>\n",
       "      <td>5.389036</td>\n",
       "      <td>11.271672</td>\n",
       "      <td>1.678158</td>\n",
       "      <td>2.977458</td>\n",
       "      <td>0.659782</td>\n",
       "      <td>4.251222</td>\n",
       "      <td>1.553357</td>\n",
       "      <td>5.939371</td>\n",
       "      <td>4.620631</td>\n",
       "      <td>17.209243</td>\n",
       "      <td>5.795145</td>\n",
       "      <td>23.720365</td>\n",
       "      <td>8.329439</td>\n",
       "      <td>25.973607</td>\n",
       "      <td>15.287847</td>\n",
       "      <td>13.258032</td>\n",
       "      <td>8.344457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20497</th>\n",
       "      <td>-0.237936</td>\n",
       "      <td>-0.278151</td>\n",
       "      <td>-0.255565</td>\n",
       "      <td>-0.221896</td>\n",
       "      <td>-0.215083</td>\n",
       "      <td>-0.222839</td>\n",
       "      <td>-0.215396</td>\n",
       "      <td>-0.216464</td>\n",
       "      <td>-0.164038</td>\n",
       "      <td>-0.234033</td>\n",
       "      <td>-0.113020</td>\n",
       "      <td>-0.158623</td>\n",
       "      <td>-0.173323</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.181631</td>\n",
       "      <td>-0.128616</td>\n",
       "      <td>-0.193653</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.128032</td>\n",
       "      <td>-0.126883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20498</th>\n",
       "      <td>0.027078</td>\n",
       "      <td>-0.042573</td>\n",
       "      <td>-0.134075</td>\n",
       "      <td>-0.018481</td>\n",
       "      <td>-0.031766</td>\n",
       "      <td>-0.108912</td>\n",
       "      <td>-0.077528</td>\n",
       "      <td>-0.168777</td>\n",
       "      <td>-0.056130</td>\n",
       "      <td>-0.021716</td>\n",
       "      <td>-0.021378</td>\n",
       "      <td>-0.047679</td>\n",
       "      <td>-0.033593</td>\n",
       "      <td>-0.027739</td>\n",
       "      <td>-0.004701</td>\n",
       "      <td>-0.026959</td>\n",
       "      <td>-0.007284</td>\n",
       "      <td>-0.036701</td>\n",
       "      <td>-0.008642</td>\n",
       "      <td>-0.041510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20499</th>\n",
       "      <td>0.453567</td>\n",
       "      <td>0.437581</td>\n",
       "      <td>1.209685</td>\n",
       "      <td>1.395503</td>\n",
       "      <td>0.577594</td>\n",
       "      <td>0.390636</td>\n",
       "      <td>0.969745</td>\n",
       "      <td>0.724169</td>\n",
       "      <td>0.130132</td>\n",
       "      <td>1.101128</td>\n",
       "      <td>0.687686</td>\n",
       "      <td>1.011803</td>\n",
       "      <td>1.250106</td>\n",
       "      <td>0.438208</td>\n",
       "      <td>2.004382</td>\n",
       "      <td>0.868224</td>\n",
       "      <td>1.729439</td>\n",
       "      <td>1.635334</td>\n",
       "      <td>0.923658</td>\n",
       "      <td>1.121414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20500</th>\n",
       "      <td>-0.025755</td>\n",
       "      <td>0.065084</td>\n",
       "      <td>0.054222</td>\n",
       "      <td>0.044495</td>\n",
       "      <td>0.196145</td>\n",
       "      <td>-0.073413</td>\n",
       "      <td>-0.040898</td>\n",
       "      <td>-0.130096</td>\n",
       "      <td>0.093666</td>\n",
       "      <td>-0.143986</td>\n",
       "      <td>-0.025048</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>-0.085424</td>\n",
       "      <td>-0.005774</td>\n",
       "      <td>0.108933</td>\n",
       "      <td>0.086321</td>\n",
       "      <td>0.145991</td>\n",
       "      <td>0.056075</td>\n",
       "      <td>0.068434</td>\n",
       "      <td>0.100185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20501</th>\n",
       "      <td>-0.097900</td>\n",
       "      <td>-0.072428</td>\n",
       "      <td>-0.118451</td>\n",
       "      <td>-0.077416</td>\n",
       "      <td>-0.044985</td>\n",
       "      <td>-0.055250</td>\n",
       "      <td>-0.106227</td>\n",
       "      <td>-0.066906</td>\n",
       "      <td>-0.011937</td>\n",
       "      <td>-0.036192</td>\n",
       "      <td>-0.068062</td>\n",
       "      <td>-0.100705</td>\n",
       "      <td>-0.094848</td>\n",
       "      <td>-0.024678</td>\n",
       "      <td>-0.085475</td>\n",
       "      <td>-0.068171</td>\n",
       "      <td>-0.089972</td>\n",
       "      <td>-0.084648</td>\n",
       "      <td>-0.076894</td>\n",
       "      <td>-0.076024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20502 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sample0   Sample1   Sample2   Sample3    Sample4   Sample5   Sample6  \\\n",
       "0     -0.231762 -0.247491 -0.248976 -0.202346  -0.186685 -0.223989 -0.179934   \n",
       "1     -0.238106 -0.284280 -0.262749 -0.237539  -0.216971 -0.231590 -0.217126   \n",
       "2     -0.237680 -0.283885 -0.262749 -0.237539  -0.216971 -0.240506 -0.195062   \n",
       "3     -0.205081 -0.257032 -0.221823 -0.210507  -0.196147 -0.212986 -0.202461   \n",
       "4      2.161295  7.610784  4.325499  5.389036  11.271672  1.678158  2.977458   \n",
       "...         ...       ...       ...       ...        ...       ...       ...   \n",
       "20497 -0.237936 -0.278151 -0.255565 -0.221896  -0.215083 -0.222839 -0.215396   \n",
       "20498  0.027078 -0.042573 -0.134075 -0.018481  -0.031766 -0.108912 -0.077528   \n",
       "20499  0.453567  0.437581  1.209685  1.395503   0.577594  0.390636  0.969745   \n",
       "20500 -0.025755  0.065084  0.054222  0.044495   0.196145 -0.073413 -0.040898   \n",
       "20501 -0.097900 -0.072428 -0.118451 -0.077416  -0.044985 -0.055250 -0.106227   \n",
       "\n",
       "        Sample7   Sample8   Sample9  Sample10  Sample11   Sample12  Sample13  \\\n",
       "0     -0.214987 -0.166885 -0.280402 -0.089822 -0.124037  -0.114895 -0.098585   \n",
       "1     -0.219176 -0.167156 -0.327269 -0.113020 -0.158831  -0.173408 -0.107497   \n",
       "2     -0.219176 -0.170003 -0.327269 -0.112947 -0.158831  -0.173323 -0.107497   \n",
       "3     -0.184733 -0.146118 -0.242611 -0.104094 -0.150289  -0.155821 -0.093080   \n",
       "4      0.659782  4.251222  1.553357  5.939371  4.620631  17.209243  5.795145   \n",
       "...         ...       ...       ...       ...       ...        ...       ...   \n",
       "20497 -0.216464 -0.164038 -0.234033 -0.113020 -0.158623  -0.173323 -0.107497   \n",
       "20498 -0.168777 -0.056130 -0.021716 -0.021378 -0.047679  -0.033593 -0.027739   \n",
       "20499  0.724169  0.130132  1.101128  0.687686  1.011803   1.250106  0.438208   \n",
       "20500 -0.130096  0.093666 -0.143986 -0.025048  0.000924  -0.085424 -0.005774   \n",
       "20501 -0.066906 -0.011937 -0.036192 -0.068062 -0.100705  -0.094848 -0.024678   \n",
       "\n",
       "        Sample14  Sample15   Sample16   Sample17   Sample18  Sample19  \n",
       "0      -0.166281 -0.111795  -0.179398  -0.124769  -0.115809 -0.115490  \n",
       "1      -0.181686 -0.128897  -0.193744  -0.130680  -0.128032 -0.127016  \n",
       "2      -0.181631 -0.128968  -0.193653  -0.130680  -0.127868 -0.127016  \n",
       "3      -0.171468 -0.120538  -0.180182  -0.123729  -0.118418 -0.117917  \n",
       "4      23.720365  8.329439  25.973607  15.287847  13.258032  8.344457  \n",
       "...          ...       ...        ...        ...        ...       ...  \n",
       "20497  -0.181631 -0.128616  -0.193653  -0.130680  -0.128032 -0.126883  \n",
       "20498  -0.004701 -0.026959  -0.007284  -0.036701  -0.008642 -0.041510  \n",
       "20499   2.004382  0.868224   1.729439   1.635334   0.923658  1.121414  \n",
       "20500   0.108933  0.086321   0.145991   0.056075   0.068434  0.100185  \n",
       "20501  -0.085475 -0.068171  -0.089972  -0.084648  -0.076894 -0.076024  \n",
       "\n",
       "[20502 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#z-scoring\n",
    "df_data = StandByCol(df_data,unused_cols=[])\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20492</th>\n",
       "      <th>20493</th>\n",
       "      <th>20494</th>\n",
       "      <th>20495</th>\n",
       "      <th>20496</th>\n",
       "      <th>20497</th>\n",
       "      <th>20498</th>\n",
       "      <th>20499</th>\n",
       "      <th>20500</th>\n",
       "      <th>20501</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sample0</th>\n",
       "      <td>-0.231762</td>\n",
       "      <td>-0.238106</td>\n",
       "      <td>-0.237680</td>\n",
       "      <td>-0.205081</td>\n",
       "      <td>2.161295</td>\n",
       "      <td>-0.238021</td>\n",
       "      <td>-0.206202</td>\n",
       "      <td>-0.237850</td>\n",
       "      <td>-0.238106</td>\n",
       "      <td>-0.079438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184203</td>\n",
       "      <td>-0.111937</td>\n",
       "      <td>-0.228407</td>\n",
       "      <td>-0.166812</td>\n",
       "      <td>-0.004315</td>\n",
       "      <td>-0.237936</td>\n",
       "      <td>0.027078</td>\n",
       "      <td>0.453567</td>\n",
       "      <td>-0.025755</td>\n",
       "      <td>-0.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample1</th>\n",
       "      <td>-0.247491</td>\n",
       "      <td>-0.284280</td>\n",
       "      <td>-0.283885</td>\n",
       "      <td>-0.257032</td>\n",
       "      <td>7.610784</td>\n",
       "      <td>-0.283885</td>\n",
       "      <td>-0.106731</td>\n",
       "      <td>-0.284181</td>\n",
       "      <td>-0.284280</td>\n",
       "      <td>-0.098823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223402</td>\n",
       "      <td>-0.196099</td>\n",
       "      <td>-0.265300</td>\n",
       "      <td>-0.132830</td>\n",
       "      <td>0.077738</td>\n",
       "      <td>-0.278151</td>\n",
       "      <td>-0.042573</td>\n",
       "      <td>0.437581</td>\n",
       "      <td>0.065084</td>\n",
       "      <td>-0.072428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample2</th>\n",
       "      <td>-0.248976</td>\n",
       "      <td>-0.262749</td>\n",
       "      <td>-0.262749</td>\n",
       "      <td>-0.221823</td>\n",
       "      <td>4.325499</td>\n",
       "      <td>-0.262749</td>\n",
       "      <td>-0.166221</td>\n",
       "      <td>-0.262479</td>\n",
       "      <td>-0.262390</td>\n",
       "      <td>-0.103365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121692</td>\n",
       "      <td>0.020729</td>\n",
       "      <td>-0.246496</td>\n",
       "      <td>-0.163796</td>\n",
       "      <td>-0.034224</td>\n",
       "      <td>-0.255565</td>\n",
       "      <td>-0.134075</td>\n",
       "      <td>1.209685</td>\n",
       "      <td>0.054222</td>\n",
       "      <td>-0.118451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample3</th>\n",
       "      <td>-0.202346</td>\n",
       "      <td>-0.237539</td>\n",
       "      <td>-0.237539</td>\n",
       "      <td>-0.210507</td>\n",
       "      <td>5.389036</td>\n",
       "      <td>-0.236385</td>\n",
       "      <td>-0.103795</td>\n",
       "      <td>-0.237020</td>\n",
       "      <td>-0.237193</td>\n",
       "      <td>-0.090807</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133908</td>\n",
       "      <td>-0.055885</td>\n",
       "      <td>-0.227784</td>\n",
       "      <td>-0.153899</td>\n",
       "      <td>-0.009187</td>\n",
       "      <td>-0.221896</td>\n",
       "      <td>-0.018481</td>\n",
       "      <td>1.395503</td>\n",
       "      <td>0.044495</td>\n",
       "      <td>-0.077416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample4</th>\n",
       "      <td>-0.186685</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.196147</td>\n",
       "      <td>11.271672</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.180656</td>\n",
       "      <td>-0.216535</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.058639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155550</td>\n",
       "      <td>-0.180366</td>\n",
       "      <td>-0.197942</td>\n",
       "      <td>-0.083043</td>\n",
       "      <td>0.097078</td>\n",
       "      <td>-0.215083</td>\n",
       "      <td>-0.031766</td>\n",
       "      <td>0.577594</td>\n",
       "      <td>0.196145</td>\n",
       "      <td>-0.044985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample5</th>\n",
       "      <td>-0.223989</td>\n",
       "      <td>-0.231590</td>\n",
       "      <td>-0.240506</td>\n",
       "      <td>-0.212986</td>\n",
       "      <td>1.678158</td>\n",
       "      <td>-0.231508</td>\n",
       "      <td>-0.142264</td>\n",
       "      <td>-0.239928</td>\n",
       "      <td>-0.240424</td>\n",
       "      <td>-0.064497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138837</td>\n",
       "      <td>-0.021732</td>\n",
       "      <td>-0.220941</td>\n",
       "      <td>-0.086622</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>-0.222839</td>\n",
       "      <td>-0.108912</td>\n",
       "      <td>0.390636</td>\n",
       "      <td>-0.073413</td>\n",
       "      <td>-0.055250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample6</th>\n",
       "      <td>-0.179934</td>\n",
       "      <td>-0.217126</td>\n",
       "      <td>-0.195062</td>\n",
       "      <td>-0.202461</td>\n",
       "      <td>2.977458</td>\n",
       "      <td>-0.216982</td>\n",
       "      <td>-0.068587</td>\n",
       "      <td>-0.216982</td>\n",
       "      <td>-0.215828</td>\n",
       "      <td>-0.056906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143433</td>\n",
       "      <td>0.109083</td>\n",
       "      <td>-0.206166</td>\n",
       "      <td>-0.056041</td>\n",
       "      <td>-0.039889</td>\n",
       "      <td>-0.215396</td>\n",
       "      <td>-0.077528</td>\n",
       "      <td>0.969745</td>\n",
       "      <td>-0.040898</td>\n",
       "      <td>-0.106227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample7</th>\n",
       "      <td>-0.214987</td>\n",
       "      <td>-0.219176</td>\n",
       "      <td>-0.219176</td>\n",
       "      <td>-0.184733</td>\n",
       "      <td>0.659782</td>\n",
       "      <td>-0.144678</td>\n",
       "      <td>-0.100471</td>\n",
       "      <td>-0.219073</td>\n",
       "      <td>-0.219176</td>\n",
       "      <td>-0.047668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150555</td>\n",
       "      <td>-0.089982</td>\n",
       "      <td>-0.216106</td>\n",
       "      <td>-0.181722</td>\n",
       "      <td>-0.076986</td>\n",
       "      <td>-0.216464</td>\n",
       "      <td>-0.168777</td>\n",
       "      <td>0.724169</td>\n",
       "      <td>-0.130096</td>\n",
       "      <td>-0.066906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample8</th>\n",
       "      <td>-0.166885</td>\n",
       "      <td>-0.167156</td>\n",
       "      <td>-0.170003</td>\n",
       "      <td>-0.146118</td>\n",
       "      <td>4.251222</td>\n",
       "      <td>-0.169867</td>\n",
       "      <td>-0.132317</td>\n",
       "      <td>-0.169867</td>\n",
       "      <td>-0.169867</td>\n",
       "      <td>-0.063451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076218</td>\n",
       "      <td>-0.101544</td>\n",
       "      <td>-0.155091</td>\n",
       "      <td>-0.057622</td>\n",
       "      <td>0.114271</td>\n",
       "      <td>-0.164038</td>\n",
       "      <td>-0.056130</td>\n",
       "      <td>0.130132</td>\n",
       "      <td>0.093666</td>\n",
       "      <td>-0.011937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample9</th>\n",
       "      <td>-0.280402</td>\n",
       "      <td>-0.327269</td>\n",
       "      <td>-0.327269</td>\n",
       "      <td>-0.242611</td>\n",
       "      <td>1.553357</td>\n",
       "      <td>-0.320154</td>\n",
       "      <td>-0.015009</td>\n",
       "      <td>-0.325306</td>\n",
       "      <td>-0.327269</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032596</td>\n",
       "      <td>0.130652</td>\n",
       "      <td>-0.305841</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>0.112169</td>\n",
       "      <td>-0.234033</td>\n",
       "      <td>-0.021716</td>\n",
       "      <td>1.101128</td>\n",
       "      <td>-0.143986</td>\n",
       "      <td>-0.036192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample10</th>\n",
       "      <td>-0.089822</td>\n",
       "      <td>-0.113020</td>\n",
       "      <td>-0.112947</td>\n",
       "      <td>-0.104094</td>\n",
       "      <td>5.939371</td>\n",
       "      <td>-0.113057</td>\n",
       "      <td>-0.053858</td>\n",
       "      <td>-0.112617</td>\n",
       "      <td>-0.113057</td>\n",
       "      <td>-0.053051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102086</td>\n",
       "      <td>-0.106855</td>\n",
       "      <td>-0.108579</td>\n",
       "      <td>-0.084210</td>\n",
       "      <td>-0.020093</td>\n",
       "      <td>-0.113020</td>\n",
       "      <td>-0.021378</td>\n",
       "      <td>0.687686</td>\n",
       "      <td>-0.025048</td>\n",
       "      <td>-0.068062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample11</th>\n",
       "      <td>-0.124037</td>\n",
       "      <td>-0.158831</td>\n",
       "      <td>-0.158831</td>\n",
       "      <td>-0.150289</td>\n",
       "      <td>4.620631</td>\n",
       "      <td>-0.158779</td>\n",
       "      <td>-0.079526</td>\n",
       "      <td>-0.158363</td>\n",
       "      <td>-0.158831</td>\n",
       "      <td>-0.083221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140674</td>\n",
       "      <td>-0.140098</td>\n",
       "      <td>-0.154096</td>\n",
       "      <td>-0.117670</td>\n",
       "      <td>-0.028790</td>\n",
       "      <td>-0.158623</td>\n",
       "      <td>-0.047679</td>\n",
       "      <td>1.011803</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>-0.100705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample12</th>\n",
       "      <td>-0.114895</td>\n",
       "      <td>-0.173408</td>\n",
       "      <td>-0.173323</td>\n",
       "      <td>-0.155821</td>\n",
       "      <td>17.209243</td>\n",
       "      <td>-0.173408</td>\n",
       "      <td>-0.106242</td>\n",
       "      <td>-0.172809</td>\n",
       "      <td>-0.173408</td>\n",
       "      <td>-0.086795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145394</td>\n",
       "      <td>-0.145308</td>\n",
       "      <td>-0.167411</td>\n",
       "      <td>-0.130658</td>\n",
       "      <td>-0.079170</td>\n",
       "      <td>-0.173323</td>\n",
       "      <td>-0.033593</td>\n",
       "      <td>1.250106</td>\n",
       "      <td>-0.085424</td>\n",
       "      <td>-0.094848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample13</th>\n",
       "      <td>-0.098585</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.093080</td>\n",
       "      <td>5.795145</td>\n",
       "      <td>-0.107227</td>\n",
       "      <td>-0.090213</td>\n",
       "      <td>-0.107407</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.063747</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087062</td>\n",
       "      <td>-0.094264</td>\n",
       "      <td>-0.103086</td>\n",
       "      <td>-0.080581</td>\n",
       "      <td>-0.029899</td>\n",
       "      <td>-0.107497</td>\n",
       "      <td>-0.027739</td>\n",
       "      <td>0.438208</td>\n",
       "      <td>-0.005774</td>\n",
       "      <td>-0.024678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample14</th>\n",
       "      <td>-0.166281</td>\n",
       "      <td>-0.181686</td>\n",
       "      <td>-0.181631</td>\n",
       "      <td>-0.171468</td>\n",
       "      <td>23.720365</td>\n",
       "      <td>-0.181576</td>\n",
       "      <td>-0.058183</td>\n",
       "      <td>-0.181190</td>\n",
       "      <td>-0.181686</td>\n",
       "      <td>-0.083159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156549</td>\n",
       "      <td>-0.169005</td>\n",
       "      <td>-0.168068</td>\n",
       "      <td>-0.103614</td>\n",
       "      <td>0.016746</td>\n",
       "      <td>-0.181631</td>\n",
       "      <td>-0.004701</td>\n",
       "      <td>2.004382</td>\n",
       "      <td>0.108933</td>\n",
       "      <td>-0.085475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample15</th>\n",
       "      <td>-0.111795</td>\n",
       "      <td>-0.128897</td>\n",
       "      <td>-0.128968</td>\n",
       "      <td>-0.120538</td>\n",
       "      <td>8.329439</td>\n",
       "      <td>-0.122768</td>\n",
       "      <td>-0.066974</td>\n",
       "      <td>-0.128756</td>\n",
       "      <td>-0.128968</td>\n",
       "      <td>-0.053588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105659</td>\n",
       "      <td>-0.090855</td>\n",
       "      <td>-0.121782</td>\n",
       "      <td>-0.087051</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>-0.128616</td>\n",
       "      <td>-0.026959</td>\n",
       "      <td>0.868224</td>\n",
       "      <td>0.086321</td>\n",
       "      <td>-0.068171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample16</th>\n",
       "      <td>-0.179398</td>\n",
       "      <td>-0.193744</td>\n",
       "      <td>-0.193653</td>\n",
       "      <td>-0.180182</td>\n",
       "      <td>25.973607</td>\n",
       "      <td>-0.193744</td>\n",
       "      <td>-0.098497</td>\n",
       "      <td>-0.193378</td>\n",
       "      <td>-0.193744</td>\n",
       "      <td>-0.086855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162301</td>\n",
       "      <td>-0.171835</td>\n",
       "      <td>-0.182010</td>\n",
       "      <td>-0.124349</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>-0.193653</td>\n",
       "      <td>-0.007284</td>\n",
       "      <td>1.729439</td>\n",
       "      <td>0.145991</td>\n",
       "      <td>-0.089972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample17</th>\n",
       "      <td>-0.124769</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.123729</td>\n",
       "      <td>15.287847</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.062481</td>\n",
       "      <td>-0.130188</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.066586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115847</td>\n",
       "      <td>-0.123017</td>\n",
       "      <td>-0.125152</td>\n",
       "      <td>-0.077807</td>\n",
       "      <td>-0.003641</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.036701</td>\n",
       "      <td>1.635334</td>\n",
       "      <td>0.056075</td>\n",
       "      <td>-0.084648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample18</th>\n",
       "      <td>-0.115809</td>\n",
       "      <td>-0.128032</td>\n",
       "      <td>-0.127868</td>\n",
       "      <td>-0.118418</td>\n",
       "      <td>13.258032</td>\n",
       "      <td>-0.127909</td>\n",
       "      <td>-0.052269</td>\n",
       "      <td>-0.127868</td>\n",
       "      <td>-0.128032</td>\n",
       "      <td>-0.060765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108138</td>\n",
       "      <td>-0.120029</td>\n",
       "      <td>-0.122820</td>\n",
       "      <td>-0.092818</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>-0.128032</td>\n",
       "      <td>-0.008642</td>\n",
       "      <td>0.923658</td>\n",
       "      <td>0.068434</td>\n",
       "      <td>-0.076894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample19</th>\n",
       "      <td>-0.115490</td>\n",
       "      <td>-0.127016</td>\n",
       "      <td>-0.127016</td>\n",
       "      <td>-0.117917</td>\n",
       "      <td>8.344457</td>\n",
       "      <td>-0.126927</td>\n",
       "      <td>-0.053948</td>\n",
       "      <td>-0.126883</td>\n",
       "      <td>-0.127016</td>\n",
       "      <td>-0.072159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108272</td>\n",
       "      <td>-0.103608</td>\n",
       "      <td>-0.122352</td>\n",
       "      <td>-0.084374</td>\n",
       "      <td>-0.017347</td>\n",
       "      <td>-0.126883</td>\n",
       "      <td>-0.041510</td>\n",
       "      <td>1.121414</td>\n",
       "      <td>0.100185</td>\n",
       "      <td>-0.076024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 20502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3          4         5      \\\n",
       "Sample0  -0.231762 -0.238106 -0.237680 -0.205081   2.161295 -0.238021   \n",
       "Sample1  -0.247491 -0.284280 -0.283885 -0.257032   7.610784 -0.283885   \n",
       "Sample2  -0.248976 -0.262749 -0.262749 -0.221823   4.325499 -0.262749   \n",
       "Sample3  -0.202346 -0.237539 -0.237539 -0.210507   5.389036 -0.236385   \n",
       "Sample4  -0.186685 -0.216971 -0.216971 -0.196147  11.271672 -0.216971   \n",
       "Sample5  -0.223989 -0.231590 -0.240506 -0.212986   1.678158 -0.231508   \n",
       "Sample6  -0.179934 -0.217126 -0.195062 -0.202461   2.977458 -0.216982   \n",
       "Sample7  -0.214987 -0.219176 -0.219176 -0.184733   0.659782 -0.144678   \n",
       "Sample8  -0.166885 -0.167156 -0.170003 -0.146118   4.251222 -0.169867   \n",
       "Sample9  -0.280402 -0.327269 -0.327269 -0.242611   1.553357 -0.320154   \n",
       "Sample10 -0.089822 -0.113020 -0.112947 -0.104094   5.939371 -0.113057   \n",
       "Sample11 -0.124037 -0.158831 -0.158831 -0.150289   4.620631 -0.158779   \n",
       "Sample12 -0.114895 -0.173408 -0.173323 -0.155821  17.209243 -0.173408   \n",
       "Sample13 -0.098585 -0.107497 -0.107497 -0.093080   5.795145 -0.107227   \n",
       "Sample14 -0.166281 -0.181686 -0.181631 -0.171468  23.720365 -0.181576   \n",
       "Sample15 -0.111795 -0.128897 -0.128968 -0.120538   8.329439 -0.122768   \n",
       "Sample16 -0.179398 -0.193744 -0.193653 -0.180182  25.973607 -0.193744   \n",
       "Sample17 -0.124769 -0.130680 -0.130680 -0.123729  15.287847 -0.130680   \n",
       "Sample18 -0.115809 -0.128032 -0.127868 -0.118418  13.258032 -0.127909   \n",
       "Sample19 -0.115490 -0.127016 -0.127016 -0.117917   8.344457 -0.126927   \n",
       "\n",
       "             6         7         8         9      ...     20492     20493  \\\n",
       "Sample0  -0.206202 -0.237850 -0.238106 -0.079438  ... -0.184203 -0.111937   \n",
       "Sample1  -0.106731 -0.284181 -0.284280 -0.098823  ... -0.223402 -0.196099   \n",
       "Sample2  -0.166221 -0.262479 -0.262390 -0.103365  ... -0.121692  0.020729   \n",
       "Sample3  -0.103795 -0.237020 -0.237193 -0.090807  ... -0.133908 -0.055885   \n",
       "Sample4  -0.180656 -0.216535 -0.216971 -0.058639  ... -0.155550 -0.180366   \n",
       "Sample5  -0.142264 -0.239928 -0.240424 -0.064497  ... -0.138837 -0.021732   \n",
       "Sample6  -0.068587 -0.216982 -0.215828 -0.056906  ... -0.143433  0.109083   \n",
       "Sample7  -0.100471 -0.219073 -0.219176 -0.047668  ... -0.150555 -0.089982   \n",
       "Sample8  -0.132317 -0.169867 -0.169867 -0.063451  ... -0.076218 -0.101544   \n",
       "Sample9  -0.015009 -0.325306 -0.327269 -0.002087  ... -0.032596  0.130652   \n",
       "Sample10 -0.053858 -0.112617 -0.113057 -0.053051  ... -0.102086 -0.106855   \n",
       "Sample11 -0.079526 -0.158363 -0.158831 -0.083221  ... -0.140674 -0.140098   \n",
       "Sample12 -0.106242 -0.172809 -0.173408 -0.086795  ... -0.145394 -0.145308   \n",
       "Sample13 -0.090213 -0.107407 -0.107497 -0.063747  ... -0.087062 -0.094264   \n",
       "Sample14 -0.058183 -0.181190 -0.181686 -0.083159  ... -0.156549 -0.169005   \n",
       "Sample15 -0.066974 -0.128756 -0.128968 -0.053588  ... -0.105659 -0.090855   \n",
       "Sample16 -0.098497 -0.193378 -0.193744 -0.086855  ... -0.162301 -0.171835   \n",
       "Sample17 -0.062481 -0.130188 -0.130680 -0.066586  ... -0.115847 -0.123017   \n",
       "Sample18 -0.052269 -0.127868 -0.128032 -0.060765  ... -0.108138 -0.120029   \n",
       "Sample19 -0.053948 -0.126883 -0.127016 -0.072159  ... -0.108272 -0.103608   \n",
       "\n",
       "             20494     20495     20496     20497     20498     20499  \\\n",
       "Sample0  -0.228407 -0.166812 -0.004315 -0.237936  0.027078  0.453567   \n",
       "Sample1  -0.265300 -0.132830  0.077738 -0.278151 -0.042573  0.437581   \n",
       "Sample2  -0.246496 -0.163796 -0.034224 -0.255565 -0.134075  1.209685   \n",
       "Sample3  -0.227784 -0.153899 -0.009187 -0.221896 -0.018481  1.395503   \n",
       "Sample4  -0.197942 -0.083043  0.097078 -0.215083 -0.031766  0.577594   \n",
       "Sample5  -0.220941 -0.086622  0.008318 -0.222839 -0.108912  0.390636   \n",
       "Sample6  -0.206166 -0.056041 -0.039889 -0.215396 -0.077528  0.969745   \n",
       "Sample7  -0.216106 -0.181722 -0.076986 -0.216464 -0.168777  0.724169   \n",
       "Sample8  -0.155091 -0.057622  0.114271 -0.164038 -0.056130  0.130132   \n",
       "Sample9  -0.305841 -0.196902  0.112169 -0.234033 -0.021716  1.101128   \n",
       "Sample10 -0.108579 -0.084210 -0.020093 -0.113020 -0.021378  0.687686   \n",
       "Sample11 -0.154096 -0.117670 -0.028790 -0.158623 -0.047679  1.011803   \n",
       "Sample12 -0.167411 -0.130658 -0.079170 -0.173323 -0.033593  1.250106   \n",
       "Sample13 -0.103086 -0.080581 -0.029899 -0.107497 -0.027739  0.438208   \n",
       "Sample14 -0.168068 -0.103614  0.016746 -0.181631 -0.004701  2.004382   \n",
       "Sample15 -0.121782 -0.087051 -0.001034 -0.128616 -0.026959  0.868224   \n",
       "Sample16 -0.182010 -0.124349  0.003258 -0.193653 -0.007284  1.729439   \n",
       "Sample17 -0.125152 -0.077807 -0.003641 -0.130680 -0.036701  1.635334   \n",
       "Sample18 -0.122820 -0.092818  0.016352 -0.128032 -0.008642  0.923658   \n",
       "Sample19 -0.122352 -0.084374 -0.017347 -0.126883 -0.041510  1.121414   \n",
       "\n",
       "             20500     20501  \n",
       "Sample0  -0.025755 -0.097900  \n",
       "Sample1   0.065084 -0.072428  \n",
       "Sample2   0.054222 -0.118451  \n",
       "Sample3   0.044495 -0.077416  \n",
       "Sample4   0.196145 -0.044985  \n",
       "Sample5  -0.073413 -0.055250  \n",
       "Sample6  -0.040898 -0.106227  \n",
       "Sample7  -0.130096 -0.066906  \n",
       "Sample8   0.093666 -0.011937  \n",
       "Sample9  -0.143986 -0.036192  \n",
       "Sample10 -0.025048 -0.068062  \n",
       "Sample11  0.000924 -0.100705  \n",
       "Sample12 -0.085424 -0.094848  \n",
       "Sample13 -0.005774 -0.024678  \n",
       "Sample14  0.108933 -0.085475  \n",
       "Sample15  0.086321 -0.068171  \n",
       "Sample16  0.145991 -0.089972  \n",
       "Sample17  0.056075 -0.084648  \n",
       "Sample18  0.068434 -0.076894  \n",
       "Sample19  0.100185 -0.076024  \n",
       "\n",
       "[20 rows x 20502 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = df_data.T\n",
    "x_all = df_data.values  \n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno = pd.read_csv('input_data_2.csv')\n",
    "anno_ids = df_anno.id.values\n",
    "anno_class = df_anno.label.values\n",
    "class_type = df_anno.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sample0', 'Sample1', 'Sample2', 'Sample3', 'Sample4', 'Sample5',\n",
       "       'Sample6', 'Sample7', 'Sample8', 'Sample9', 'Sample10', 'Sample11',\n",
       "       'Sample12', 'Sample13', 'Sample14', 'Sample15', 'Sample16',\n",
       "       'Sample17', 'Sample18', 'Sample19'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Tumor', 'Tumor', 'Tumor', 'Tumor', 'Tumor', 'Tumor', 'Tumor',\n",
       "       'Tumor', 'Tumor', 'Tumor', 'Normal', 'Normal', 'Normal', 'Normal',\n",
       "       'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Tumor', 'Normal'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cls = len(class_type)\n",
    "y_all = []\n",
    "for id_ in id_names:\n",
    "    idx = np.where(anno_ids == id_)\n",
    "    y_all.append(np.where(class_type == anno_class[idx])[0][0])\n",
    "y_all = np.asarray(y_all)\n",
    "y_all = to_categorical(y_all, num_classes=n_cls)\n",
    "\n",
    "y_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to train and val\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_all, y_all, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.48976301e-01, -2.62748842e-01, -2.62748842e-01, ...,\n",
       "         1.20968516e+00,  5.42222426e-02, -1.18450672e-01],\n",
       "       [-1.66281463e-01, -1.81686327e-01, -1.81631186e-01, ...,\n",
       "         2.00438175e+00,  1.08932969e-01, -8.54748492e-02],\n",
       "       [-1.86684604e-01, -2.16971028e-01, -2.16971028e-01, ...,\n",
       "         5.77593876e-01,  1.96144627e-01, -4.49847619e-02],\n",
       "       ...,\n",
       "       [-1.14894803e-01, -1.73408296e-01, -1.73322625e-01, ...,\n",
       "         1.25010571e+00, -8.54238940e-02, -9.48477255e-02],\n",
       "       [-1.24037335e-01, -1.58831134e-01, -1.58831134e-01, ...,\n",
       "         1.01180311e+00,  9.23805117e-04, -1.00705319e-01],\n",
       "       [-2.23988538e-01, -2.31590305e-01, -2.40506371e-01, ...,\n",
       "         3.90635687e-01, -7.34126509e-02, -5.52502900e-02]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = WxHyperParameter(epochs=30, learning_ratio=0.01, batch_size=8, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0491 - accuracy: 0.6250\n",
      "Epoch 00001: val_loss improved from inf to 1.79270, saving model to ./slp_wx_weights_best.hdf5\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 1.6983 - accuracy: 0.5000 - val_loss: 1.7927 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7792 - accuracy: 0.5000\n",
      "Epoch 00002: val_loss improved from 1.79270 to 0.00000, saving model to ./slp_wx_weights_best.hdf5\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.4275 - accuracy: 0.7500 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1332e-04 - accuracy: 1.0000\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 5.6658e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.4703e-08 - accuracy: 1.0000\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 2.2352e-08 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00005: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00006: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00007: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00008: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00009: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00010: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00011: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00012: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00013: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00014: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00015: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00016: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00017: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00018: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00019: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00020: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00021: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 22/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00022: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 23/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00023: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 24/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00024: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 25/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00025: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 26/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00026: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 27/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00027: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00028: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 29/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00029: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 30/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00030: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "1/1 [==============================] - 0s 767us/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "\n",
      "function [wx_slp] finished in 2.8156471252441406 s\n"
     ]
    }
   ],
   "source": [
    "sel_idx, sel_weight, val_acc = wx_slp(x_train, y_train, x_val, y_val, n_selection=10, hyper_param=hp, num_cls=n_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single Layer WX\n",
      "selected feature names: ['SFTPC' 'SFTPA2' 'SFTPA1' 'FTL' 'EEF1A1' 'ACTB' 'GAPDH' 'ACTG1' 'HLA-B'\n",
      " 'FGG']\n",
      "selected feature index: [15871 15869 15868  6668  5350   200  6827   203  7870  6380]\n",
      "selected feature weight: [27.63028513 18.62849216  5.74430402  3.84302923  2.83739353  2.40131759\n",
      "  1.24756945  1.1751258   0.95742483  0.81372882]\n",
      "evaluation accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print ('\\nSingle Layer WX')\n",
    "print ('selected feature names:',f_names[sel_idx])\n",
    "print ('selected feature index:',sel_idx)\n",
    "print ('selected feature weight:',sel_weight)\n",
    "print ('evaluation accuracy:',val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20502"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_genes = len(f_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4235 - accuracy: 0.7500\n",
      "Epoch 00001: val_loss improved from inf to 2.24856, saving model to ./slp_wx_weights_best.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1801 - accuracy: 0.5625 - val_loss: 2.2486 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.2524 - accuracy: 0.6250\n",
      "Epoch 00002: val_loss improved from 2.24856 to 0.00000, saving model to ./slp_wx_weights_best.hdf5\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.0208 - accuracy: 0.6875 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0916e-04 - accuracy: 1.0000\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.5458e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00005: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00006: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00007: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00008: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00009: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00010: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00011: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00012: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00013: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00014: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00015: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00016: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00017: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00018: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00019: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00020: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00021: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 22/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00022: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 23/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00023: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 24/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00024: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 25/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00025: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 26/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00026: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 27/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00027: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00028: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 29/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00029: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 30/30\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00030: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "1/1 [==============================] - 0s 689us/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "\n",
      "function [wx_slp] finished in 1.2418487071990967 s\n"
     ]
    }
   ],
   "source": [
    "sel_idx, sel_weight, val_acc = wx_slp(x_train, y_train, x_val, y_val, n_selection=len(f_names), hyper_param=hp, num_cls=n_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_RDS(file_path):\n",
    "\n",
    "    read_RDS = robjects.r['readRDS']\n",
    "    return read_RDS(file_path)\n",
    "\n",
    "\n",
    "\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "def pandas_to_r(df):\n",
    "\n",
    "    with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "        r_from_pandas_df = robjects.conversion.py2rpy(df)\n",
    "    return r_from_pandas_df\n",
    "\n",
    "\n",
    "\n",
    "def r_to_pandas(df):\n",
    "    with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "            pandas_from_r_df = robjects.conversion.rpy2py(df)\n",
    "    return pandas_from_r_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = r_to_pandas(load_RDS(\"/home/colombelli/Documents/datasets/research/brca.rds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = main[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_class = df['class'].values\n",
    "class_type = df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(anno_class):\n",
    "    \n",
    "    anno_y = []\n",
    "    for cls in anno_class:\n",
    "        new_y = [0, 0]\n",
    "        new_y[int(cls)] = 1\n",
    "        anno_y.append(new_y)\n",
    "        \n",
    "    return np.array(anno_y, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = to_categorical(anno_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCGB2A2</th>\n",
       "      <th>SCGB1D2</th>\n",
       "      <th>MUCL1</th>\n",
       "      <th>TFF1</th>\n",
       "      <th>PIP</th>\n",
       "      <th>GSTM1</th>\n",
       "      <th>CYP2B7P1</th>\n",
       "      <th>CPB1</th>\n",
       "      <th>TFAP2B</th>\n",
       "      <th>LTF</th>\n",
       "      <th>...</th>\n",
       "      <th>PPIG</th>\n",
       "      <th>LOC100125556</th>\n",
       "      <th>EXOC4</th>\n",
       "      <th>TEC</th>\n",
       "      <th>TUSC2</th>\n",
       "      <th>MYOM3</th>\n",
       "      <th>TRIT1</th>\n",
       "      <th>OTUD7A</th>\n",
       "      <th>USP27X</th>\n",
       "      <th>ARPM1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA.A7.A0CE.11A.21R.A089.07</th>\n",
       "      <td>10.554105</td>\n",
       "      <td>9.403274</td>\n",
       "      <td>11.540931</td>\n",
       "      <td>10.885613</td>\n",
       "      <td>12.365593</td>\n",
       "      <td>7.116029</td>\n",
       "      <td>4.282285</td>\n",
       "      <td>8.959008</td>\n",
       "      <td>4.681972</td>\n",
       "      <td>10.166929</td>\n",
       "      <td>...</td>\n",
       "      <td>5.796821</td>\n",
       "      <td>2.920358</td>\n",
       "      <td>5.514571</td>\n",
       "      <td>1.463288</td>\n",
       "      <td>5.777505</td>\n",
       "      <td>2.904490</td>\n",
       "      <td>4.777675</td>\n",
       "      <td>1.081973</td>\n",
       "      <td>3.198914</td>\n",
       "      <td>2.086866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA.A7.A0CH.11A.32R.A089.07</th>\n",
       "      <td>12.073295</td>\n",
       "      <td>10.504623</td>\n",
       "      <td>13.597325</td>\n",
       "      <td>5.261387</td>\n",
       "      <td>13.478214</td>\n",
       "      <td>7.906778</td>\n",
       "      <td>4.471297</td>\n",
       "      <td>6.055256</td>\n",
       "      <td>4.302780</td>\n",
       "      <td>9.480826</td>\n",
       "      <td>...</td>\n",
       "      <td>4.963855</td>\n",
       "      <td>2.261438</td>\n",
       "      <td>5.410830</td>\n",
       "      <td>1.598669</td>\n",
       "      <td>5.715822</td>\n",
       "      <td>0.936055</td>\n",
       "      <td>4.207983</td>\n",
       "      <td>1.269942</td>\n",
       "      <td>2.587507</td>\n",
       "      <td>1.942952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA.A7.A0D9.11A.53R.A089.07</th>\n",
       "      <td>4.652332</td>\n",
       "      <td>3.530492</td>\n",
       "      <td>1.508318</td>\n",
       "      <td>0.426305</td>\n",
       "      <td>1.940593</td>\n",
       "      <td>7.007359</td>\n",
       "      <td>1.914802</td>\n",
       "      <td>3.613250</td>\n",
       "      <td>0.692070</td>\n",
       "      <td>5.399014</td>\n",
       "      <td>...</td>\n",
       "      <td>5.841457</td>\n",
       "      <td>1.566242</td>\n",
       "      <td>6.175997</td>\n",
       "      <td>1.566785</td>\n",
       "      <td>5.381756</td>\n",
       "      <td>0.291109</td>\n",
       "      <td>3.960901</td>\n",
       "      <td>1.176532</td>\n",
       "      <td>2.412553</td>\n",
       "      <td>2.647025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA.A7.A0DB.11A.33R.A089.07</th>\n",
       "      <td>1.761845</td>\n",
       "      <td>1.067840</td>\n",
       "      <td>2.404918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502656</td>\n",
       "      <td>6.964555</td>\n",
       "      <td>1.093856</td>\n",
       "      <td>3.814888</td>\n",
       "      <td>0.014930</td>\n",
       "      <td>4.048145</td>\n",
       "      <td>...</td>\n",
       "      <td>5.429606</td>\n",
       "      <td>1.782776</td>\n",
       "      <td>5.905556</td>\n",
       "      <td>2.404466</td>\n",
       "      <td>5.122230</td>\n",
       "      <td>0.423852</td>\n",
       "      <td>3.972255</td>\n",
       "      <td>1.239671</td>\n",
       "      <td>2.236906</td>\n",
       "      <td>2.082817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA.A7.A0DC.11A.41R.A089.07</th>\n",
       "      <td>10.252504</td>\n",
       "      <td>8.455516</td>\n",
       "      <td>10.585970</td>\n",
       "      <td>4.566540</td>\n",
       "      <td>8.184018</td>\n",
       "      <td>6.626465</td>\n",
       "      <td>3.178531</td>\n",
       "      <td>10.345035</td>\n",
       "      <td>3.593233</td>\n",
       "      <td>10.683944</td>\n",
       "      <td>...</td>\n",
       "      <td>5.811546</td>\n",
       "      <td>2.560664</td>\n",
       "      <td>6.023154</td>\n",
       "      <td>1.526697</td>\n",
       "      <td>5.571172</td>\n",
       "      <td>1.779448</td>\n",
       "      <td>4.574964</td>\n",
       "      <td>1.429833</td>\n",
       "      <td>2.755122</td>\n",
       "      <td>2.044148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA.A2.A0YF.01A.21R.A109.07</th>\n",
       "      <td>5.038334</td>\n",
       "      <td>1.253711</td>\n",
       "      <td>3.693359</td>\n",
       "      <td>12.327187</td>\n",
       "      <td>10.168611</td>\n",
       "      <td>8.107207</td>\n",
       "      <td>5.954676</td>\n",
       "      <td>6.593731</td>\n",
       "      <td>0.306234</td>\n",
       "      <td>9.413047</td>\n",
       "      <td>...</td>\n",
       "      <td>6.534514</td>\n",
       "      <td>3.496705</td>\n",
       "      <td>6.282955</td>\n",
       "      <td>1.672766</td>\n",
       "      <td>6.086834</td>\n",
       "      <td>0.523894</td>\n",
       "      <td>4.025191</td>\n",
       "      <td>0.320993</td>\n",
       "      <td>3.962603</td>\n",
       "      <td>3.188679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA.A2.A0YG.01A.21R.A109.07</th>\n",
       "      <td>8.100245</td>\n",
       "      <td>3.338998</td>\n",
       "      <td>9.162506</td>\n",
       "      <td>7.940139</td>\n",
       "      <td>9.660221</td>\n",
       "      <td>2.253116</td>\n",
       "      <td>6.509444</td>\n",
       "      <td>9.613153</td>\n",
       "      <td>0.904108</td>\n",
       "      <td>8.791196</td>\n",
       "      <td>...</td>\n",
       "      <td>6.433270</td>\n",
       "      <td>3.134508</td>\n",
       "      <td>6.636783</td>\n",
       "      <td>1.467468</td>\n",
       "      <td>5.966175</td>\n",
       "      <td>0.706581</td>\n",
       "      <td>4.452151</td>\n",
       "      <td>0.623535</td>\n",
       "      <td>1.953326</td>\n",
       "      <td>1.895817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA.A2.A0YH.01A.11R.A109.07</th>\n",
       "      <td>6.922006</td>\n",
       "      <td>5.330330</td>\n",
       "      <td>5.801106</td>\n",
       "      <td>9.149674</td>\n",
       "      <td>4.247552</td>\n",
       "      <td>3.544134</td>\n",
       "      <td>8.185333</td>\n",
       "      <td>8.434137</td>\n",
       "      <td>0.088184</td>\n",
       "      <td>5.861760</td>\n",
       "      <td>...</td>\n",
       "      <td>5.410977</td>\n",
       "      <td>3.980932</td>\n",
       "      <td>6.022863</td>\n",
       "      <td>1.552861</td>\n",
       "      <td>6.021822</td>\n",
       "      <td>0.525002</td>\n",
       "      <td>4.555630</td>\n",
       "      <td>0.256139</td>\n",
       "      <td>3.301557</td>\n",
       "      <td>1.914174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA.A2.A0YI.01A.31R.A10J.07</th>\n",
       "      <td>8.072254</td>\n",
       "      <td>6.823353</td>\n",
       "      <td>3.746149</td>\n",
       "      <td>9.944818</td>\n",
       "      <td>9.986013</td>\n",
       "      <td>7.478888</td>\n",
       "      <td>7.067757</td>\n",
       "      <td>15.659412</td>\n",
       "      <td>1.628264</td>\n",
       "      <td>13.565433</td>\n",
       "      <td>...</td>\n",
       "      <td>6.234599</td>\n",
       "      <td>2.542349</td>\n",
       "      <td>5.627656</td>\n",
       "      <td>1.630827</td>\n",
       "      <td>5.738051</td>\n",
       "      <td>0.889373</td>\n",
       "      <td>5.179193</td>\n",
       "      <td>0.533942</td>\n",
       "      <td>3.761526</td>\n",
       "      <td>2.004443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA.A2.A0YJ.01A.11R.A109.07</th>\n",
       "      <td>1.521557</td>\n",
       "      <td>0.440845</td>\n",
       "      <td>0.146275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.909141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050662</td>\n",
       "      <td>0.063802</td>\n",
       "      <td>5.758947</td>\n",
       "      <td>...</td>\n",
       "      <td>5.653244</td>\n",
       "      <td>3.000919</td>\n",
       "      <td>6.481432</td>\n",
       "      <td>1.988913</td>\n",
       "      <td>5.121630</td>\n",
       "      <td>0.159572</td>\n",
       "      <td>4.885904</td>\n",
       "      <td>0.228398</td>\n",
       "      <td>2.609747</td>\n",
       "      <td>1.839580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 14021 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                SCGB2A2    SCGB1D2      MUCL1       TFF1  \\\n",
       "TCGA.A7.A0CE.11A.21R.A089.07  10.554105   9.403274  11.540931  10.885613   \n",
       "TCGA.A7.A0CH.11A.32R.A089.07  12.073295  10.504623  13.597325   5.261387   \n",
       "TCGA.A7.A0D9.11A.53R.A089.07   4.652332   3.530492   1.508318   0.426305   \n",
       "TCGA.A7.A0DB.11A.33R.A089.07   1.761845   1.067840   2.404918   0.000000   \n",
       "TCGA.A7.A0DC.11A.41R.A089.07  10.252504   8.455516  10.585970   4.566540   \n",
       "...                                 ...        ...        ...        ...   \n",
       "TCGA.A2.A0YF.01A.21R.A109.07   5.038334   1.253711   3.693359  12.327187   \n",
       "TCGA.A2.A0YG.01A.21R.A109.07   8.100245   3.338998   9.162506   7.940139   \n",
       "TCGA.A2.A0YH.01A.11R.A109.07   6.922006   5.330330   5.801106   9.149674   \n",
       "TCGA.A2.A0YI.01A.31R.A10J.07   8.072254   6.823353   3.746149   9.944818   \n",
       "TCGA.A2.A0YJ.01A.11R.A109.07   1.521557   0.440845   0.146275   0.000000   \n",
       "\n",
       "                                    PIP     GSTM1  CYP2B7P1       CPB1  \\\n",
       "TCGA.A7.A0CE.11A.21R.A089.07  12.365593  7.116029  4.282285   8.959008   \n",
       "TCGA.A7.A0CH.11A.32R.A089.07  13.478214  7.906778  4.471297   6.055256   \n",
       "TCGA.A7.A0D9.11A.53R.A089.07   1.940593  7.007359  1.914802   3.613250   \n",
       "TCGA.A7.A0DB.11A.33R.A089.07   0.502656  6.964555  1.093856   3.814888   \n",
       "TCGA.A7.A0DC.11A.41R.A089.07   8.184018  6.626465  3.178531  10.345035   \n",
       "...                                 ...       ...       ...        ...   \n",
       "TCGA.A2.A0YF.01A.21R.A109.07  10.168611  8.107207  5.954676   6.593731   \n",
       "TCGA.A2.A0YG.01A.21R.A109.07   9.660221  2.253116  6.509444   9.613153   \n",
       "TCGA.A2.A0YH.01A.11R.A109.07   4.247552  3.544134  8.185333   8.434137   \n",
       "TCGA.A2.A0YI.01A.31R.A10J.07   9.986013  7.478888  7.067757  15.659412   \n",
       "TCGA.A2.A0YJ.01A.11R.A109.07   0.000000  5.909141  0.000000   0.050662   \n",
       "\n",
       "                                TFAP2B        LTF  ...      PPIG  \\\n",
       "TCGA.A7.A0CE.11A.21R.A089.07  4.681972  10.166929  ...  5.796821   \n",
       "TCGA.A7.A0CH.11A.32R.A089.07  4.302780   9.480826  ...  4.963855   \n",
       "TCGA.A7.A0D9.11A.53R.A089.07  0.692070   5.399014  ...  5.841457   \n",
       "TCGA.A7.A0DB.11A.33R.A089.07  0.014930   4.048145  ...  5.429606   \n",
       "TCGA.A7.A0DC.11A.41R.A089.07  3.593233  10.683944  ...  5.811546   \n",
       "...                                ...        ...  ...       ...   \n",
       "TCGA.A2.A0YF.01A.21R.A109.07  0.306234   9.413047  ...  6.534514   \n",
       "TCGA.A2.A0YG.01A.21R.A109.07  0.904108   8.791196  ...  6.433270   \n",
       "TCGA.A2.A0YH.01A.11R.A109.07  0.088184   5.861760  ...  5.410977   \n",
       "TCGA.A2.A0YI.01A.31R.A10J.07  1.628264  13.565433  ...  6.234599   \n",
       "TCGA.A2.A0YJ.01A.11R.A109.07  0.063802   5.758947  ...  5.653244   \n",
       "\n",
       "                              LOC100125556     EXOC4       TEC     TUSC2  \\\n",
       "TCGA.A7.A0CE.11A.21R.A089.07      2.920358  5.514571  1.463288  5.777505   \n",
       "TCGA.A7.A0CH.11A.32R.A089.07      2.261438  5.410830  1.598669  5.715822   \n",
       "TCGA.A7.A0D9.11A.53R.A089.07      1.566242  6.175997  1.566785  5.381756   \n",
       "TCGA.A7.A0DB.11A.33R.A089.07      1.782776  5.905556  2.404466  5.122230   \n",
       "TCGA.A7.A0DC.11A.41R.A089.07      2.560664  6.023154  1.526697  5.571172   \n",
       "...                                    ...       ...       ...       ...   \n",
       "TCGA.A2.A0YF.01A.21R.A109.07      3.496705  6.282955  1.672766  6.086834   \n",
       "TCGA.A2.A0YG.01A.21R.A109.07      3.134508  6.636783  1.467468  5.966175   \n",
       "TCGA.A2.A0YH.01A.11R.A109.07      3.980932  6.022863  1.552861  6.021822   \n",
       "TCGA.A2.A0YI.01A.31R.A10J.07      2.542349  5.627656  1.630827  5.738051   \n",
       "TCGA.A2.A0YJ.01A.11R.A109.07      3.000919  6.481432  1.988913  5.121630   \n",
       "\n",
       "                                 MYOM3     TRIT1    OTUD7A    USP27X     ARPM1  \n",
       "TCGA.A7.A0CE.11A.21R.A089.07  2.904490  4.777675  1.081973  3.198914  2.086866  \n",
       "TCGA.A7.A0CH.11A.32R.A089.07  0.936055  4.207983  1.269942  2.587507  1.942952  \n",
       "TCGA.A7.A0D9.11A.53R.A089.07  0.291109  3.960901  1.176532  2.412553  2.647025  \n",
       "TCGA.A7.A0DB.11A.33R.A089.07  0.423852  3.972255  1.239671  2.236906  2.082817  \n",
       "TCGA.A7.A0DC.11A.41R.A089.07  1.779448  4.574964  1.429833  2.755122  2.044148  \n",
       "...                                ...       ...       ...       ...       ...  \n",
       "TCGA.A2.A0YF.01A.21R.A109.07  0.523894  4.025191  0.320993  3.962603  3.188679  \n",
       "TCGA.A2.A0YG.01A.21R.A109.07  0.706581  4.452151  0.623535  1.953326  1.895817  \n",
       "TCGA.A2.A0YH.01A.11R.A109.07  0.525002  4.555630  0.256139  3.301557  1.914174  \n",
       "TCGA.A2.A0YI.01A.31R.A10J.07  0.889373  5.179193  0.533942  3.761526  2.004443  \n",
       "TCGA.A2.A0YJ.01A.11R.A109.07  0.159572  4.885904  0.228398  2.609747  1.839580  \n",
       "\n",
       "[200 rows x 14021 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.55410472,  9.40327405, 11.54093124, ...,  1.08197275,\n",
       "         3.1989141 ,  2.08686602],\n",
       "       [12.07329462, 10.50462343, 13.59732499, ...,  1.26994219,\n",
       "         2.58750748,  1.94295236],\n",
       "       [ 4.65233205,  3.53049232,  1.5083182 , ...,  1.17653231,\n",
       "         2.41255306,  2.64702489],\n",
       "       ...,\n",
       "       [ 6.92200564,  5.33032974,  5.80110554, ...,  0.25613937,\n",
       "         3.30155738,  1.91417361],\n",
       "       [ 8.07225401,  6.82335279,  3.74614941, ...,  0.53394192,\n",
       "         3.76152636,  2.00444257],\n",
       "       [ 1.5215571 ,  0.44084534,  0.14627491, ...,  0.22839752,\n",
       "         2.60974696,  1.83957963]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all = df.iloc[:, 0:-1].values\n",
    "x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 4.1003 - accuracy: 0.3750\n",
      "Epoch 00001: val_loss improved from inf to 0.37606, saving model to ./slp_wx_weights_best.hdf5\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 58.1593 - accuracy: 0.7650 - val_loss: 0.3761 - val_accuracy: 0.9850 - lr: 0.0010\n",
      "Epoch 2/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00002: val_loss did not improve from 0.37606\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.9089 - accuracy: 0.9450 - val_loss: 2.1261 - val_accuracy: 0.9750 - lr: 0.0010\n",
      "Epoch 3/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00003: val_loss did not improve from 0.37606\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4712 - accuracy: 0.9900 - val_loss: 1.7333 - val_accuracy: 0.9800 - lr: 0.0010\n",
      "Epoch 4/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 1.5076 - accuracy: 0.8750\n",
      "Epoch 00004: val_loss did not improve from 0.37606\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6152 - accuracy: 0.9750 - val_loss: 1.6688 - val_accuracy: 0.9850 - lr: 0.0010\n",
      "Epoch 5/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00005: val_loss did not improve from 0.37606\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5904 - accuracy: 0.9900 - val_loss: 0.4517 - val_accuracy: 0.9900 - lr: 0.0010\n",
      "Epoch 6/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00006: val_loss did not improve from 0.37606\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7061 - accuracy: 0.9900 - val_loss: 2.6082 - val_accuracy: 0.9800 - lr: 0.0010\n",
      "Epoch 7/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00007: val_loss improved from 0.37606 to 0.00377, saving model to ./slp_wx_weights_best.hdf5\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6478 - accuracy: 0.9800 - val_loss: 0.0038 - val_accuracy: 0.9950 - lr: 0.0010\n",
      "Epoch 8/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00008: val_loss did not improve from 0.00377\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0820 - accuracy: 0.9850 - val_loss: 2.1083 - val_accuracy: 0.9550 - lr: 0.0010\n",
      "Epoch 9/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00009: val_loss improved from 0.00377 to 0.00000, saving model to ./slp_wx_weights_best.hdf5\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.4089 - accuracy: 0.9250 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 10/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00010: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1407 - accuracy: 0.9800 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 11/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00011: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0430 - accuracy: 0.9950 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00012: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00013: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00014: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00015: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00016: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00017: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00018: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00019: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00020: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00021: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 22/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00022: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 23/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00023: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 24/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00024: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 25/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00025: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 26/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00026: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 27/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00027: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00028: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 29/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00029: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 30/30\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 00030: val_loss did not improve from 0.00000\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "7/7 [==============================] - 0s 923us/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "\n",
      "function [wx_slp] finished in 3.012053966522217 s\n"
     ]
    }
   ],
   "source": [
    "sel_idx, sel_weight, val_acc = wx_slp(x_all, y_true, x_all, y_true, n_selection=len(features), hyper_param=hp, num_cls=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rank(df, sel_idx):\n",
    "    \n",
    "    print(\"Processing data...\")\n",
    "    genes = list(df.columns)\n",
    "    data = {}\n",
    "    data['gene'] = []\n",
    "    data['rank'] = []\n",
    "    for i, gen_id in enumerate(sel_idx):\n",
    "        data['gene'].append(genes[gen_id])\n",
    "        data['rank'].append(i+1)\n",
    "\n",
    "    rank = pd.DataFrame(data, columns=['rank']).set_index(pd.Index(data['gene']))\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FN1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMP11</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRYAB</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COL1A1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGN</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FABP4</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAA1</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KRT14</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SFRP1</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIR4508</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APOD</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KRT5</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MUCL1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COL1A2</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COL3A1</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rank\n",
       "FN1         1\n",
       "MMP11       2\n",
       "CRYAB       3\n",
       "COL1A1      4\n",
       "BGN         5\n",
       "FABP4       6\n",
       "SAA1        7\n",
       "KRT14       8\n",
       "SFRP1       9\n",
       "MIR4508    10\n",
       "APOD       11\n",
       "KRT5       12\n",
       "MUCL1      13\n",
       "COL1A2     14\n",
       "COL3A1     15"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_rank(df, sel_idx)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
